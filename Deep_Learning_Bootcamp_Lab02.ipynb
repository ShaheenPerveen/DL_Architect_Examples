{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DL_Architect_Day01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4j9_7AotGndr",
        "colab_type": "code",
        "outputId": "60b729fa-bcb8-4c1d-a005-5efb6890e4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# loading all libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing, metrics, cross_validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.constraints import maxnorm\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.constraints import maxnorm\n",
        "from keras.layers import Dropout, BatchNormalization\n",
        "from pylab import rcParams\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "D6uFWdjfryIX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0G2vb0x8ryBi",
        "colab_type": "code",
        "outputId": "b0ebaad9-c9cb-4d31-d73a-811eae4180fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y1Br4RxfL6oj",
        "colab_type": "code",
        "outputId": "39604d93-afcd-4cb9-f441-67e432311651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "BackOrders = pd.read_csv(\"/content/drive/My Drive/Colab_Folder/BackOrders.csv\", na_values=[\"?\",\",\"])\n",
        "BackOrders.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sku</th>\n",
              "      <th>national_inv</th>\n",
              "      <th>lead_time</th>\n",
              "      <th>in_transit_qty</th>\n",
              "      <th>forecast_3_month</th>\n",
              "      <th>forecast_6_month</th>\n",
              "      <th>forecast_9_month</th>\n",
              "      <th>sales_1_month</th>\n",
              "      <th>sales_3_month</th>\n",
              "      <th>sales_6_month</th>\n",
              "      <th>...</th>\n",
              "      <th>pieces_past_due</th>\n",
              "      <th>perf_6_month_avg</th>\n",
              "      <th>perf_12_month_avg</th>\n",
              "      <th>local_bo_qty</th>\n",
              "      <th>deck_risk</th>\n",
              "      <th>oe_constraint</th>\n",
              "      <th>ppap_risk</th>\n",
              "      <th>stop_auto_buy</th>\n",
              "      <th>rev_stop</th>\n",
              "      <th>went_on_backorder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1888279</td>\n",
              "      <td>117</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1870557</td>\n",
              "      <td>7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1475481</td>\n",
              "      <td>258</td>\n",
              "      <td>15.0</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>77</td>\n",
              "      <td>184</td>\n",
              "      <td>46</td>\n",
              "      <td>132</td>\n",
              "      <td>256</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1758220</td>\n",
              "      <td>46</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1360312</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sku  national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
              "0  1888279           117        NaN               0                 0   \n",
              "1  1870557             7        2.0               0                 0   \n",
              "2  1475481           258       15.0              10                10   \n",
              "3  1758220            46        2.0               0                 0   \n",
              "4  1360312             2        2.0               0                 4   \n",
              "\n",
              "   forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
              "0                 0                 0              0              0   \n",
              "1                 0                 0              0              0   \n",
              "2                77               184             46            132   \n",
              "3                 0                 0              1              2   \n",
              "4                 6                10              2              2   \n",
              "\n",
              "   sales_6_month        ...         pieces_past_due  perf_6_month_avg  \\\n",
              "0             15        ...                       0            -99.00   \n",
              "1              0        ...                       0              0.50   \n",
              "2            256        ...                       0              0.54   \n",
              "3              6        ...                       0              0.75   \n",
              "4              5        ...                       0              0.97   \n",
              "\n",
              "  perf_12_month_avg  local_bo_qty  deck_risk  oe_constraint  ppap_risk  \\\n",
              "0            -99.00             0         No             No        Yes   \n",
              "1              0.28             0        Yes             No         No   \n",
              "2              0.70             0         No             No         No   \n",
              "3              0.90             0        Yes             No         No   \n",
              "4              0.92             0         No             No         No   \n",
              "\n",
              "  stop_auto_buy rev_stop went_on_backorder  \n",
              "0           Yes       No                No  \n",
              "1           Yes       No                No  \n",
              "2           Yes       No                No  \n",
              "3           Yes       No                No  \n",
              "4           Yes       No                No  \n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "Z1ALbUsTMxjW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BackOrders.describe() # missing values in lead_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiUhfqVmM4JD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exploratory analysis to impute lead time"
      ]
    },
    {
      "metadata": {
        "id": "6rOia2s3NVpk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = [\"went_on_backorder\", \"deck_risk\", \"oe_constraint\", \"ppap_risk\", \"stop_auto_buy\", \"rev_stop\"]\n",
        "\n",
        "for col in columns:\n",
        "  plt.figure()\n",
        "  sns.boxplot(x=\"lead_time\", y=col, data=BackOrders)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3sAoBtwO7Zz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluating fields hich can be used to impute lead time NA values"
      ]
    },
    {
      "metadata": {
        "id": "qLWARTG8O3r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(BackOrders.groupby('stop_auto_buy', as_index=False)['lead_time'].mean())\n",
        "print(BackOrders.groupby('rev_stop', as_index=False)['lead_time'].mean())\n",
        "print(BackOrders.groupby('oe_constraint', as_index=False)['lead_time'].mean())\n",
        "print(BackOrders.groupby(['stop_auto_buy', 'oe_constraint'], as_index=False)['lead_time'].mean())\n",
        "print(BackOrders.groupby(['stop_auto_buy', 'rev_stop'], as_index=False)['lead_time'].mean())\n",
        "print(BackOrders.groupby(['rev_stop', 'oe_constraint'], as_index=False)['lead_time'].mean())\n",
        "print(BackOrders.groupby(['stop_auto_buy', 'rev_stop', 'oe_constraint'], as_index=False)['lead_time'].mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-UV1wJYPKUG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imputing lead_time"
      ]
    },
    {
      "metadata": {
        "id": "rPVmxtyiPIWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "BackOrders['lead_time'] = BackOrders.groupby(['stop_auto_buy', 'rev_stop', 'oe_constraint'])['lead_time'].transform(lambda x: x.fillna(x.mean()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I3dCZVj5Pc_U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Changing datatype of objects column to categories for dummification later on"
      ]
    },
    {
      "metadata": {
        "id": "CWseSxNVPd6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def dtype_Convert(dtp1, col, data):\n",
        "    for i in range(len(dtp1)):\n",
        "        if dtp1[i] == \"object\":\n",
        "            data[col[i]]=data[col[i]].astype('category')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tAAJ-sWGpUa3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtp = pd.Series(BackOrders.dtypes)\n",
        "dtp1 = dtp.tolist()\n",
        "col = BackOrders.columns.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBQH_7_noI7j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtype_Convert(dtp1, col, BackOrders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbJim4Aephka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Outlier detection and treatment"
      ]
    },
    {
      "metadata": {
        "id": "ga4xY5k-pg87",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cols = list(BackOrders.select_dtypes(include=['int', 'float']).columns)\n",
        "\n",
        "for col in cols:\n",
        "  plt.figure()\n",
        "  sns.boxplot(x=col, y=\"went_on_backorder\", data=BackOrders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kn2wIdf6pg6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cols = list(BackOrders.select_dtypes(include=['int', 'float']).columns)\n",
        "\n",
        "cols = [e for e in cols if e not in (\"local_bo_qty\", \"lead_time\", \"sku\")]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svihfXUPpyVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using quantile methods for outier treatment.\n",
        "Outlier treatment on new data to compare with old dataset"
      ]
    },
    {
      "metadata": {
        "id": "XtT_VUxIpx9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BackOrders_NoOut = pd.read_csv(\"/content/drive/My Drive/Colab_Folder/BackOrders.csv\", na_values=[\"?\",\",\"])\n",
        "BackOrders_NoOut['lead_time'] = BackOrders_NoOut.groupby(['stop_auto_buy', 'rev_stop', 'oe_constraint'])['lead_time'].transform(lambda x: x.fillna(x.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9V9zAy8Npg4R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def outliers_iqr(cols, df):\n",
        "  for col in cols:\n",
        "    quartile_1 = df[col].quantile(0.25) \n",
        "    quartile_3 = df[col].quantile(0.75)\n",
        "    iqr = quartile_3 - quartile_1\n",
        "    lower_bound = quartile_1 - (iqr * 1.5)\n",
        "    upper_bound = quartile_3 + (iqr * 1.5)\n",
        "    median = df[col].median()\n",
        "    df[col] = df[col].mask(df[col] > upper_bound, median)\n",
        "    df[col] = df[col].mask(df[col] < lower_bound, median)   \n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tTwO2eUbpg1K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "outliers_iqr(cols, BackOrders_NoOut)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BsC96-XIqyaA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Comparing summary to check effect of outlier treatment"
      ]
    },
    {
      "metadata": {
        "id": "kfQgtGQapgyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(pd.concat([pd.DataFrame(BackOrders.describe()), pd.DataFrame(BackOrders_NoOut.describe())]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItUXsIPvYdf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapping_dictionary = {\"went_on_backorder\":{ \"Yes\": 1, \"No\": 0}}\n",
        "BackOrders_NoOut1 = BackOrders_NoOut.replace(mapping_dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FhzuL047oPIn",
        "colab_type": "code",
        "outputId": "25b31832-eac7-496b-8e50-f0aff8ed2a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "cell_type": "code",
      "source": [
        "#Plotting the frequencies of orders which went into backorder and which did not\n",
        "count_classes = pd.value_counts(BackOrders_NoOut1['went_on_backorder'], sort = True)\n",
        "print(count_classes)\n",
        "\n",
        "#Drawing a barplot\n",
        "count_classes.plot(kind = 'bar', rot=0)\n",
        "\n",
        "#Giving titles and labels to the plot\n",
        "plt.title(\"Transaction class distribution\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\");"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    50296\n",
            "1    11293\n",
            "Name: went_on_backorder, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFnCAYAAACsMZCtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb234253eb8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "k5HrX75_ZTDN",
        "colab_type": "code",
        "outputId": "e3666e85-d7a9-4d1d-cedb-0e0159fdefbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print(BackOrders_NoOut1.shape)\n",
        "print(BackOrders_NoOut.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(61589, 23)\n",
            "(61589, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k_JCyQ4stIJL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BackOrders_NoOut2 = pd.get_dummies(BackOrders_NoOut1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0hC672IsWRiB",
        "colab_type": "code",
        "outputId": "13f726a0-c78c-4852-8d47-170db1598c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "cell_type": "code",
      "source": [
        "BackOrders_NoOut2.describe(include = 'all')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sku</th>\n",
              "      <th>national_inv</th>\n",
              "      <th>lead_time</th>\n",
              "      <th>in_transit_qty</th>\n",
              "      <th>forecast_3_month</th>\n",
              "      <th>forecast_6_month</th>\n",
              "      <th>forecast_9_month</th>\n",
              "      <th>sales_1_month</th>\n",
              "      <th>sales_3_month</th>\n",
              "      <th>sales_6_month</th>\n",
              "      <th>...</th>\n",
              "      <th>deck_risk_No</th>\n",
              "      <th>deck_risk_Yes</th>\n",
              "      <th>oe_constraint_No</th>\n",
              "      <th>oe_constraint_Yes</th>\n",
              "      <th>ppap_risk_No</th>\n",
              "      <th>ppap_risk_Yes</th>\n",
              "      <th>stop_auto_buy_No</th>\n",
              "      <th>stop_auto_buy_Yes</th>\n",
              "      <th>rev_stop_No</th>\n",
              "      <th>rev_stop_Yes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.158900e+04</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.0</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "      <td>61589.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.037188e+06</td>\n",
              "      <td>18.053646</td>\n",
              "      <td>7.679858</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.320950</td>\n",
              "      <td>4.951923</td>\n",
              "      <td>7.165874</td>\n",
              "      <td>1.457013</td>\n",
              "      <td>4.641300</td>\n",
              "      <td>9.253958</td>\n",
              "      <td>...</td>\n",
              "      <td>0.781714</td>\n",
              "      <td>0.218286</td>\n",
              "      <td>0.999805</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.873403</td>\n",
              "      <td>0.126597</td>\n",
              "      <td>0.037117</td>\n",
              "      <td>0.962883</td>\n",
              "      <td>0.999675</td>\n",
              "      <td>0.000325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>6.564178e+05</td>\n",
              "      <td>27.157908</td>\n",
              "      <td>6.414996</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.767793</td>\n",
              "      <td>11.631048</td>\n",
              "      <td>16.642746</td>\n",
              "      <td>2.981867</td>\n",
              "      <td>8.218458</td>\n",
              "      <td>16.263241</td>\n",
              "      <td>...</td>\n",
              "      <td>0.413086</td>\n",
              "      <td>0.413086</td>\n",
              "      <td>0.013957</td>\n",
              "      <td>0.013957</td>\n",
              "      <td>0.332524</td>\n",
              "      <td>0.332524</td>\n",
              "      <td>0.189050</td>\n",
              "      <td>0.189050</td>\n",
              "      <td>0.018018</td>\n",
              "      <td>0.018018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.068628e+06</td>\n",
              "      <td>-75.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.498574e+06</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.898033e+06</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.314826e+06</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.284895e+06</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                sku  national_inv     lead_time  in_transit_qty  \\\n",
              "count  6.158900e+04  61589.000000  61589.000000         61589.0   \n",
              "mean   2.037188e+06     18.053646      7.679858             0.0   \n",
              "std    6.564178e+05     27.157908      6.414996             0.0   \n",
              "min    1.068628e+06    -75.000000      0.000000             0.0   \n",
              "25%    1.498574e+06      3.000000      4.000000             0.0   \n",
              "50%    1.898033e+06     10.000000      8.000000             0.0   \n",
              "75%    2.314826e+06     18.000000      8.000000             0.0   \n",
              "max    3.284895e+06    138.000000     52.000000             0.0   \n",
              "\n",
              "       forecast_3_month  forecast_6_month  forecast_9_month  sales_1_month  \\\n",
              "count      61589.000000      61589.000000      61589.000000   61589.000000   \n",
              "mean           2.320950          4.951923          7.165874       1.457013   \n",
              "std            5.767793         11.631048         16.642746       2.981867   \n",
              "min            0.000000          0.000000          0.000000       0.000000   \n",
              "25%            0.000000          0.000000          0.000000       0.000000   \n",
              "50%            0.000000          0.000000          0.000000       0.000000   \n",
              "75%            0.000000          2.000000          4.000000       1.000000   \n",
              "max           30.000000         62.000000         90.000000      15.000000   \n",
              "\n",
              "       sales_3_month  sales_6_month      ...       deck_risk_No  \\\n",
              "count   61589.000000   61589.000000      ...       61589.000000   \n",
              "mean        4.641300       9.253958      ...           0.781714   \n",
              "std         8.218458      16.263241      ...           0.413086   \n",
              "min         0.000000       0.000000      ...           0.000000   \n",
              "25%         0.000000       0.000000      ...           1.000000   \n",
              "50%         2.000000       4.000000      ...           1.000000   \n",
              "75%         4.000000       9.000000      ...           1.000000   \n",
              "max        42.000000      85.000000      ...           1.000000   \n",
              "\n",
              "       deck_risk_Yes  oe_constraint_No  oe_constraint_Yes  ppap_risk_No  \\\n",
              "count   61589.000000      61589.000000       61589.000000  61589.000000   \n",
              "mean        0.218286          0.999805           0.000195      0.873403   \n",
              "std         0.413086          0.013957           0.013957      0.332524   \n",
              "min         0.000000          0.000000           0.000000      0.000000   \n",
              "25%         0.000000          1.000000           0.000000      1.000000   \n",
              "50%         0.000000          1.000000           0.000000      1.000000   \n",
              "75%         0.000000          1.000000           0.000000      1.000000   \n",
              "max         1.000000          1.000000           1.000000      1.000000   \n",
              "\n",
              "       ppap_risk_Yes  stop_auto_buy_No  stop_auto_buy_Yes   rev_stop_No  \\\n",
              "count   61589.000000      61589.000000       61589.000000  61589.000000   \n",
              "mean        0.126597          0.037117           0.962883      0.999675   \n",
              "std         0.332524          0.189050           0.189050      0.018018   \n",
              "min         0.000000          0.000000           0.000000      0.000000   \n",
              "25%         0.000000          0.000000           1.000000      1.000000   \n",
              "50%         0.000000          0.000000           1.000000      1.000000   \n",
              "75%         0.000000          0.000000           1.000000      1.000000   \n",
              "max         1.000000          1.000000           1.000000      1.000000   \n",
              "\n",
              "       rev_stop_Yes  \n",
              "count  61589.000000  \n",
              "mean       0.000325  \n",
              "std        0.018018  \n",
              "min        0.000000  \n",
              "25%        0.000000  \n",
              "50%        0.000000  \n",
              "75%        0.000000  \n",
              "max        1.000000  \n",
              "\n",
              "[8 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "4PztfKGhZb2K",
        "colab_type": "code",
        "outputId": "0248ff35-5d8f-4348-f3a9-66f9121d6e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "BackOrders_NoOut2.columns"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sku', 'national_inv', 'lead_time', 'in_transit_qty',\n",
              "       'forecast_3_month', 'forecast_6_month', 'forecast_9_month',\n",
              "       'sales_1_month', 'sales_3_month', 'sales_6_month', 'sales_9_month',\n",
              "       'min_bank', 'pieces_past_due', 'perf_6_month_avg', 'perf_12_month_avg',\n",
              "       'local_bo_qty', 'went_on_backorder', 'potential_issue_No',\n",
              "       'potential_issue_Yes', 'deck_risk_No', 'deck_risk_Yes',\n",
              "       'oe_constraint_No', 'oe_constraint_Yes', 'ppap_risk_No',\n",
              "       'ppap_risk_Yes', 'stop_auto_buy_No', 'stop_auto_buy_Yes', 'rev_stop_No',\n",
              "       'rev_stop_Yes'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "yFG0U8HZwCQV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating training and test set\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yLnd-3WWRj5l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Divide in to train and test\n",
        "y=BackOrders_NoOut2[\"went_on_backorder\"]\n",
        "X=BackOrders_NoOut2.drop('went_on_backorder', axis=1)\n",
        "\n",
        "#from sklearn.model_selection import train_test_split  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ic2Usr8hRjyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FA1RP_NPanAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swraFcCXRuHM",
        "colab_type": "code",
        "outputId": "a3385f3b-9b39-4fa4-afc9-ac45fb4dff49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "JnILh0PBRuD8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train=scaler.transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBE0kekeSTJt",
        "colab_type": "code",
        "outputId": "caae3bae-9f2b-4dda-c053-916c7349e09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1]\n",
        "print(input_dim)\n",
        "# encoding_dim = 15"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "59ahWZqbSTGs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1,input_dim=input_dim, activation='sigmoid', kernel_initializer='random_normal',kernel_regularizer=\"l1\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQBbd2R6Sow8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=50, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wO0j3W3YSTDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_train, y_train)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-YDwMlRSTAf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred_class = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUDwcXULVe58",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(y_test, y_pred_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Msaug_36Ve3l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the confusion matrix\n",
        "confusion_matrix_test=metrics.confusion_matrix(y_test, y_pred_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xr8TQnWXVe08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tgSOctTvVezF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Accuracy_Test=(confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
        "TNR_Test= confusion_matrix_test[0,0]/(confusion_matrix_test[0,0] +confusion_matrix_test[0,1])\n",
        "TPR_Test= confusion_matrix_test[1,1]/(confusion_matrix_test[1,0] +confusion_matrix_test[1,1])\n",
        "\n",
        "print(\"Test TNR: \",TNR_Test)\n",
        "print(\"Test TPR: \",TPR_Test)\n",
        "print(\"Test Accuracy: \",Accuracy_Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXR30_vQVevt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_MLP = Sequential()\n",
        "model_MLP.add(Dense(4,input_dim=input_dim, activation='tanh', kernel_initializer='random_normal',kernel_regularizer=\"l1\"))\n",
        "model_MLP.add(Dense(1,activation='sigmoid'))\n",
        "model_MLP.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYn2HPE9fIbe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_MLP = Sequential()\n",
        "model_MLP.add(Dense(14,input_dim=input_dim, activation='tanh', kernel_initializer='random_normal',kernel_regularizer=\"l1\"))\n",
        "model_MLP.add(Dropout(0.2))\n",
        "model_MLP.add(Dense(4, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model_MLP.add(BatchNormalization())\n",
        "model_MLP.add(Dense(1,activation='sigmoid'))\n",
        "model_MLP.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CeuOKKDdjU5R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_MLP = Sequential()\n",
        "model_MLP.add(Dense(14,input_dim=input_dim, activation='tanh', kernel_initializer='random_normal',kernel_regularizer=\"l2\"))\n",
        "# model_MLP.add(Dropout(0.2))\n",
        "model_MLP.add(Dense(4, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "# model_MLP.add(BatchNormalization())\n",
        "model_MLP.add(Dense(1,activation='sigmoid'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b0c9qmD5ry1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_MLP.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Z6jI5MiVs7A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_MLP.fit(X_train, y_train, epochs=50, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCzcKMD4jfWw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr_values = np.logspace(-4, -1, num=5)\n",
        "lr_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZXLhxWVerrmy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr_values = [0.003]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fZXJEAyajegp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_epoch = 20\n",
        "batch_size = 32\n",
        "\n",
        "def compile_fit(model, X, Y, lr_value):\n",
        "    \n",
        "    adam = optimizers.Adam(lr=lr_value)\n",
        "    \n",
        "    model_MLP.compile(optimizer=adam, \n",
        "                    loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    history = model_MLP.fit(X, Y,\n",
        "                    epochs=nb_epoch,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                     validation_split=0.3,\n",
        "                    verbose=1)    \n",
        "    return history\n",
        "  \n",
        "hist = []\n",
        "training_loss = []\n",
        "val_loss = []\n",
        "\n",
        "\n",
        "for lr_value in lr_values:\n",
        "    print(lr_value)\n",
        "    history = (compile_fit(model_MLP, X_train, y_train, lr_value))\n",
        "  \n",
        "    hist.append(history)\n",
        "  \n",
        "    training_loss.append(history.history['loss'])\n",
        "    val_loss.append(history.history['val_loss'])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SRGsEByxn2qu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(nb_epoch))\n",
        "\n",
        "# Visualize loss history\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IoGW-XkYVs27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = model_MLP.evaluate(X_train, y_train)\n",
        "print(\"%s: %.2f%%\" % (model_MLP.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KeZ5PiUBVs08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred_class_MLP = model_MLP.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aqmj025hVsxK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(y_test,y_pred_class_MLP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dfoG0ptddokl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the confusion matrix\n",
        "confusion_matrix_test_mlp=metrics.confusion_matrix(y_test,y_pred_class_MLP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_PdQzMudoim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Accuracy_Test=(confusion_matrix_test_mlp[0,0]+confusion_matrix_test_mlp[1,1])/(confusion_matrix_test_mlp[0,0]+confusion_matrix_test_mlp[0,1]+confusion_matrix_test_mlp[1,0]+confusion_matrix_test_mlp[1,1])\n",
        "TNR_Test= confusion_matrix_test_mlp[0,0]/(confusion_matrix_test_mlp[0,0] +confusion_matrix_test_mlp[0,1])\n",
        "TPR_Test= confusion_matrix_test_mlp[1,1]/(confusion_matrix_test_mlp[1,0] +confusion_matrix_test_mlp[1,1])\n",
        "\n",
        "print(\"Test TNR: \",TNR_Test)\n",
        "print(\"Test TPR: \",TPR_Test)\n",
        "print(\"Test Accuracy: \",Accuracy_Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TpTsxweFoT5x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "using grid search approach to find the best model"
      ]
    },
    {
      "metadata": {
        "id": "m78FjXPtoaME",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import regularizers\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jkUmDRA-on_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(activation,  neurons, learn_rate, kernel_regularizer):\n",
        "    \n",
        "#     # create model\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(4, input_dim=15, kernel_initializer=init_mode, activation='tanh'))\n",
        "#     model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
        "    \n",
        "#     # Compile model\n",
        "#     sgd=optimizers.SGD(0.001)\n",
        "#     model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    print(\"learning rate \", learn_rate)\n",
        "    print(\"activation\", activation)\n",
        "    print(\"neurons in 1st layer\", neurons)\n",
        "    print(\"kernel regularizer\", kernel_regularizer)\n",
        "    # create model\n",
        "    adam = optimizers.Adam(lr=learn_rate)\n",
        "    \n",
        "    \n",
        "    model_MLP = Sequential()\n",
        "    model_MLP.add(Dense(neurons,input_dim=input_dim, activation=activation,kernel_regularizer=kernel_regularizer))\n",
        "    model_MLP.add(Dense(4, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model_MLP.add(Dense(1,activation='sigmoid'))\n",
        "    \n",
        "    # compile model\n",
        "    model_MLP.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model_MLP\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OlA1ipKHoswX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = KerasClassifier(build_fn=create_model, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5HYjDXQgdogj",
        "colab_type": "code",
        "outputId": "5dc0da5f-0fe2-4cc3-c0af-f9b7aec7f33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# init_mode = ['uniform', 'lecun_uniform']\n",
        "kernel_regularizer = ['l1', 'l2']\n",
        "activation = ['softmax', 'softplus', 'relu', 'tanh', 'sigmoid']\n",
        "# batch_size = [32, 64]\n",
        "learn_rate = [0.003, 0.006]\n",
        "# epochs=[20,30]\n",
        "neurons = [10, 14]\n",
        "# batch_size=[32]\n",
        "param_grid = dict(\n",
        "#     init_mode=init_mode,\n",
        "                  kernel_regularizer = kernel_regularizer, \n",
        "#                   batch_size=batch_size,\n",
        "                  activation = activation, \n",
        "                  learn_rate = learn_rate, neurons=neurons\n",
        "                 )\n",
        "param_grid"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['softmax', 'softplus', 'relu', 'tanh', 'sigmoid'],\n",
              " 'kernel_regularizer': ['l1', 'l2'],\n",
              " 'learn_rate': [0.003, 0.006],\n",
              " 'neurons': [10, 14]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "5FI6ty0BdodO",
        "colab_type": "code",
        "outputId": "cbe7915e-2c23-49fb-f7ee-4e2dac709db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62634
        }
      },
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, scoring='f1')\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.5447 - acc: 0.8029\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4833 - acc: 0.8129\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4834 - acc: 0.8129\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4835 - acc: 0.8129\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.4834 - acc: 0.8129\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.4833 - acc: 0.8129\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.4835 - acc: 0.8129\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4834 - acc: 0.8129\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4834 - acc: 0.8129\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4835 - acc: 0.8129\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4833 - acc: 0.8129\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4837 - acc: 0.8129\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4835 - acc: 0.8129\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.4835 - acc: 0.8129\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.4835 - acc: 0.8129\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.4344 - acc: 0.8129\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3977 - acc: 0.8334\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3855 - acc: 0.8510\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3795 - acc: 0.8566\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3762 - acc: 0.8595\n",
            "24636/24636 [==============================] - 1s 31us/step\n",
            "24635/24635 [==============================] - 1s 30us/step\n",
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.4933 - acc: 0.8181\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3896 - acc: 0.8374\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3791 - acc: 0.8530\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3737 - acc: 0.8557\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3700 - acc: 0.8583\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3686 - acc: 0.8606\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3663 - acc: 0.8622\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3657 - acc: 0.8635\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3649 - acc: 0.8644\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3632 - acc: 0.8642\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3625 - acc: 0.8658\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3620 - acc: 0.8665\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3615 - acc: 0.8667\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3615 - acc: 0.8676\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3609 - acc: 0.8691\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3617 - acc: 0.8695\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3603 - acc: 0.8686\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3606 - acc: 0.8696\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3605 - acc: 0.8684\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 4s 177us/step - loss: 0.3607 - acc: 0.8708\n",
            "24635/24635 [==============================] - 1s 31us/step\n",
            "24636/24636 [==============================] - 1s 30us/step\n",
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.5348 - acc: 0.8129\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4752 - acc: 0.8129\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.4028 - acc: 0.8257\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3868 - acc: 0.8470\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3766 - acc: 0.8544\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 4s 179us/step - loss: 0.3720 - acc: 0.8585\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3683 - acc: 0.8610\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3663 - acc: 0.8625\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3644 - acc: 0.8646\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 4s 179us/step - loss: 0.3633 - acc: 0.8670\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3619 - acc: 0.8656\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 4s 179us/step - loss: 0.3606 - acc: 0.8677\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 4s 178us/step - loss: 0.3603 - acc: 0.8686\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3593 - acc: 0.8686\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3589 - acc: 0.8687\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 4s 179us/step - loss: 0.3581 - acc: 0.8705\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 4s 179us/step - loss: 0.3578 - acc: 0.8707\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3574 - acc: 0.8697\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3565 - acc: 0.8712\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 4s 179us/step - loss: 0.3574 - acc: 0.8699\n",
            "24636/24636 [==============================] - 1s 33us/step\n",
            "24635/24635 [==============================] - 1s 30us/step\n",
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.4987 - acc: 0.8160\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3931 - acc: 0.8283\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3826 - acc: 0.8471\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3775 - acc: 0.8505\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3750 - acc: 0.8545\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3721 - acc: 0.8555\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3706 - acc: 0.8586\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3695 - acc: 0.8597\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3663 - acc: 0.8615\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3667 - acc: 0.8618\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3660 - acc: 0.8625\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3648 - acc: 0.8633\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3637 - acc: 0.8654\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3629 - acc: 0.8664\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3638 - acc: 0.8668\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3625 - acc: 0.8675\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3618 - acc: 0.8680\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3632 - acc: 0.8677\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3610 - acc: 0.8701\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3619 - acc: 0.8695\n",
            "24635/24635 [==============================] - 1s 33us/step\n",
            "24636/24636 [==============================] - 1s 30us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.4994 - acc: 0.8127\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.4018 - acc: 0.8378\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3869 - acc: 0.8535\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3813 - acc: 0.8583\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3794 - acc: 0.8600\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3776 - acc: 0.8635\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3765 - acc: 0.8644\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3759 - acc: 0.8653\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3759 - acc: 0.8661\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3766 - acc: 0.8661\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3772 - acc: 0.8658\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3769 - acc: 0.8645\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3757 - acc: 0.8659\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3775 - acc: 0.8646\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3781 - acc: 0.8647\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3758 - acc: 0.8658\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3776 - acc: 0.8653\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3770 - acc: 0.8651\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3784 - acc: 0.8662\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3770 - acc: 0.8663\n",
            "24636/24636 [==============================] - 1s 34us/step\n",
            "24635/24635 [==============================] - 1s 31us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.4325 - acc: 0.8418\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 4s 178us/step - loss: 0.3655 - acc: 0.8672\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3616 - acc: 0.8717\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3608 - acc: 0.8755\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3616 - acc: 0.8762\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3621 - acc: 0.8741\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3595 - acc: 0.8748\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 4s 177us/step - loss: 0.3624 - acc: 0.8737\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3585 - acc: 0.8751\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3595 - acc: 0.8733\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3565 - acc: 0.8751\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3558 - acc: 0.8749\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3565 - acc: 0.8739\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3522 - acc: 0.8742\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3541 - acc: 0.8751\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3521 - acc: 0.8764\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3513 - acc: 0.8756\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3540 - acc: 0.8772\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3509 - acc: 0.8755\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 4s 179us/step - loss: 0.3532 - acc: 0.8765\n",
            "24635/24635 [==============================] - 1s 36us/step\n",
            "24636/24636 [==============================] - 1s 32us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.5078 - acc: 0.8078\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.4022 - acc: 0.8341\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3887 - acc: 0.8533\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3826 - acc: 0.8583\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3790 - acc: 0.8614\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3800 - acc: 0.8613\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3790 - acc: 0.8634\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3779 - acc: 0.8628\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3771 - acc: 0.8636\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3772 - acc: 0.8639\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3794 - acc: 0.8636\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3777 - acc: 0.8641\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3771 - acc: 0.8652\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3790 - acc: 0.8641\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3766 - acc: 0.8658\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3776 - acc: 0.8654\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3790 - acc: 0.8653\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3790 - acc: 0.8654\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3784 - acc: 0.8656\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3794 - acc: 0.8651\n",
            "24636/24636 [==============================] - 1s 37us/step\n",
            "24635/24635 [==============================] - 1s 33us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.4788 - acc: 0.8192\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3903 - acc: 0.8481\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3815 - acc: 0.8561\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3743 - acc: 0.8629\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 4s 181us/step - loss: 0.3734 - acc: 0.8635\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 4s 180us/step - loss: 0.3726 - acc: 0.8640\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3704 - acc: 0.8660\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3721 - acc: 0.8680\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3711 - acc: 0.8676\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3705 - acc: 0.8693\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3700 - acc: 0.8684\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3714 - acc: 0.8693\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3701 - acc: 0.8689\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3700 - acc: 0.8685\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3715 - acc: 0.8686\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3684 - acc: 0.8711\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3694 - acc: 0.8702\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3704 - acc: 0.8694\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3708 - acc: 0.8698\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3702 - acc: 0.8697\n",
            "24635/24635 [==============================] - 1s 38us/step\n",
            "24636/24636 [==============================] - 1s 33us/step\n",
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.4207 - acc: 0.8227\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3528 - acc: 0.8573\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 4s 183us/step - loss: 0.3451 - acc: 0.8598\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3380 - acc: 0.8643\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3350 - acc: 0.8653\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3351 - acc: 0.8658\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3315 - acc: 0.8672\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 4s 183us/step - loss: 0.3307 - acc: 0.8684\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3289 - acc: 0.8695\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3264 - acc: 0.8691\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3254 - acc: 0.8708\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3250 - acc: 0.8719\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3240 - acc: 0.8712\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3233 - acc: 0.8721\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3230 - acc: 0.8715\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3240 - acc: 0.8729\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 4s 180us/step - loss: 0.3245 - acc: 0.8719\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3212 - acc: 0.8729\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3210 - acc: 0.8752\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3214 - acc: 0.8741\n",
            "24636/24636 [==============================] - 1s 40us/step\n",
            "24635/24635 [==============================] - 1s 33us/step\n",
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.4252 - acc: 0.8407\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3442 - acc: 0.8637\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3359 - acc: 0.8675\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3322 - acc: 0.8708\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3306 - acc: 0.8709\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3295 - acc: 0.8706\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3273 - acc: 0.8722\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3261 - acc: 0.8760\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3260 - acc: 0.8733\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3246 - acc: 0.8745\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3259 - acc: 0.8744\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3245 - acc: 0.8753\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3234 - acc: 0.8752\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3232 - acc: 0.8753\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3222 - acc: 0.8766\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3225 - acc: 0.8761\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3218 - acc: 0.8770\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3237 - acc: 0.8748\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3234 - acc: 0.8747\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3218 - acc: 0.8768\n",
            "24635/24635 [==============================] - 1s 41us/step\n",
            "24636/24636 [==============================] - 1s 35us/step\n",
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.4254 - acc: 0.8356\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3483 - acc: 0.8616\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3428 - acc: 0.8652\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3398 - acc: 0.8667\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 4s 183us/step - loss: 0.3373 - acc: 0.8665\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3323 - acc: 0.8679\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 4s 183us/step - loss: 0.3293 - acc: 0.8704\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3268 - acc: 0.8717\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3256 - acc: 0.8708\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 4s 183us/step - loss: 0.3238 - acc: 0.8714\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3224 - acc: 0.8730\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3215 - acc: 0.8733\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3201 - acc: 0.8738\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3212 - acc: 0.8733\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3185 - acc: 0.8744\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3177 - acc: 0.8762\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3174 - acc: 0.8765\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3152 - acc: 0.8759\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3157 - acc: 0.8779\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3171 - acc: 0.8768\n",
            "24636/24636 [==============================] - 1s 43us/step\n",
            "24635/24635 [==============================] - 1s 34us/step\n",
            "learning rate  0.003\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.4098 - acc: 0.8390\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3455 - acc: 0.8641\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3388 - acc: 0.8681\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3348 - acc: 0.8705\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3306 - acc: 0.8711\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3274 - acc: 0.8722\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3230 - acc: 0.8750\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3203 - acc: 0.8764\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3188 - acc: 0.8777\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3188 - acc: 0.8762\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3165 - acc: 0.8781\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3162 - acc: 0.8788\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3173 - acc: 0.8784\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3152 - acc: 0.8801\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3145 - acc: 0.8808\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3125 - acc: 0.8795\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 4s 182us/step - loss: 0.3116 - acc: 0.8817\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3093 - acc: 0.8815\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3092 - acc: 0.8817\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3089 - acc: 0.8832\n",
            "24635/24635 [==============================] - 1s 43us/step\n",
            "24636/24636 [==============================] - 1s 35us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3910 - acc: 0.8441\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3480 - acc: 0.8620\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3415 - acc: 0.8645\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 4s 182us/step - loss: 0.3360 - acc: 0.8667\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3346 - acc: 0.8660\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3302 - acc: 0.8709\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3326 - acc: 0.8698\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3289 - acc: 0.8738\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3309 - acc: 0.8701\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3276 - acc: 0.8710\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3277 - acc: 0.8709\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3253 - acc: 0.8738\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3295 - acc: 0.8708\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3280 - acc: 0.8727\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 4s 181us/step - loss: 0.3251 - acc: 0.8710\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3272 - acc: 0.8718\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 4s 183us/step - loss: 0.3254 - acc: 0.8751\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 4s 183us/step - loss: 0.3245 - acc: 0.8712\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3273 - acc: 0.8714\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3257 - acc: 0.8733\n",
            "24636/24636 [==============================] - 1s 45us/step\n",
            "24635/24635 [==============================] - 1s 35us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3891 - acc: 0.8410\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3515 - acc: 0.8598\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3453 - acc: 0.8609\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3419 - acc: 0.8637\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3394 - acc: 0.8656\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3375 - acc: 0.8681\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3350 - acc: 0.8694\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3321 - acc: 0.8706\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3329 - acc: 0.8711\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3295 - acc: 0.8738\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3315 - acc: 0.8718\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3299 - acc: 0.8720\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 183us/step - loss: 0.3295 - acc: 0.8748\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3283 - acc: 0.8735\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3296 - acc: 0.8749\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3271 - acc: 0.8758\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3287 - acc: 0.8720\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3330 - acc: 0.8718\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3328 - acc: 0.8745\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3281 - acc: 0.8751\n",
            "24635/24635 [==============================] - 1s 46us/step\n",
            "24636/24636 [==============================] - 1s 35us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.5150 - acc: 0.8083\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.4820 - acc: 0.8129\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.4821 - acc: 0.8129\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.4821 - acc: 0.8129\n",
            "24636/24636 [==============================] - 1s 47us/step\n",
            " 4320/24635 [====>.........................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "24635/24635 [==============================] - 1s 36us/step\n",
            "learning rate  0.006\n",
            "activation softmax\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.4057 - acc: 0.8322\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3522 - acc: 0.8588\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3431 - acc: 0.8645\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3380 - acc: 0.8680\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3350 - acc: 0.8709\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3312 - acc: 0.8706\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3295 - acc: 0.8734\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3261 - acc: 0.8733\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3256 - acc: 0.8725\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3253 - acc: 0.8736\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3221 - acc: 0.8746\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3256 - acc: 0.8741\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3234 - acc: 0.8752\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3264 - acc: 0.8750\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3215 - acc: 0.8747\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3217 - acc: 0.8764\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3219 - acc: 0.8751\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3174 - acc: 0.8778\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3210 - acc: 0.8748\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3212 - acc: 0.8760\n",
            "24635/24635 [==============================] - 1s 48us/step\n",
            "24636/24636 [==============================] - 1s 37us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 221us/step - loss: 0.4828 - acc: 0.8190\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3971 - acc: 0.8429\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3864 - acc: 0.8513\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3792 - acc: 0.8566\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3711 - acc: 0.8604\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3646 - acc: 0.8636\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3606 - acc: 0.8679\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3593 - acc: 0.8704\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3593 - acc: 0.8730\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3576 - acc: 0.8735\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3555 - acc: 0.8721\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3528 - acc: 0.8736\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3514 - acc: 0.8745\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3510 - acc: 0.8756\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3507 - acc: 0.8756\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3488 - acc: 0.8768\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3507 - acc: 0.8775\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3505 - acc: 0.8774\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3495 - acc: 0.8776\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3517 - acc: 0.8780\n",
            "24636/24636 [==============================] - 1s 50us/step\n",
            "24635/24635 [==============================] - 1s 36us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.4820 - acc: 0.8218\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3779 - acc: 0.8572\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3597 - acc: 0.8645\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3566 - acc: 0.8705\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3532 - acc: 0.8732\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3487 - acc: 0.8753\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3487 - acc: 0.8764\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3466 - acc: 0.8752\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3476 - acc: 0.8769\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3459 - acc: 0.8775\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3444 - acc: 0.8762\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3439 - acc: 0.8764\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3434 - acc: 0.8765\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3442 - acc: 0.8764\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3423 - acc: 0.8759\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3411 - acc: 0.8760\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3425 - acc: 0.8764\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3397 - acc: 0.8766\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3412 - acc: 0.8760\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3401 - acc: 0.8782\n",
            "24635/24635 [==============================] - 1s 50us/step\n",
            "24636/24636 [==============================] - 1s 37us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.5118 - acc: 0.8058\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.4033 - acc: 0.8304\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3944 - acc: 0.8397\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3812 - acc: 0.8540\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3746 - acc: 0.8581\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3743 - acc: 0.8632\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3675 - acc: 0.8669\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3661 - acc: 0.8698\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3655 - acc: 0.8718\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 184us/step - loss: 0.3620 - acc: 0.8731\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3618 - acc: 0.8755\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3609 - acc: 0.8749\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3586 - acc: 0.8769\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3583 - acc: 0.8762\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 183us/step - loss: 0.3552 - acc: 0.8782\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3559 - acc: 0.8790\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3546 - acc: 0.8775\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 185us/step - loss: 0.3531 - acc: 0.8785\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3540 - acc: 0.8805\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3518 - acc: 0.8794\n",
            "24636/24636 [==============================] - 1s 51us/step\n",
            "24635/24635 [==============================] - 1s 37us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.4914 - acc: 0.8265\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3864 - acc: 0.8534\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3755 - acc: 0.8624\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3738 - acc: 0.8624\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3716 - acc: 0.8641\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3677 - acc: 0.8655\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3694 - acc: 0.8650\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3654 - acc: 0.8667\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3654 - acc: 0.8678\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3623 - acc: 0.8693\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3610 - acc: 0.8692\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3589 - acc: 0.8691\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3607 - acc: 0.8714\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3580 - acc: 0.8707\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3574 - acc: 0.8711\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3589 - acc: 0.8710\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3580 - acc: 0.8714\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3549 - acc: 0.8710\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3529 - acc: 0.8718\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 185us/step - loss: 0.3543 - acc: 0.8720\n",
            "24635/24635 [==============================] - 1s 53us/step\n",
            "24636/24636 [==============================] - 1s 37us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 6s 229us/step - loss: 0.4454 - acc: 0.8385\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3882 - acc: 0.8594\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3847 - acc: 0.8619\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3821 - acc: 0.8609\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3763 - acc: 0.8656\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3778 - acc: 0.8655\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3782 - acc: 0.8656\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3711 - acc: 0.8672\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3690 - acc: 0.8662\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3697 - acc: 0.8664\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3656 - acc: 0.8687\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3652 - acc: 0.8676\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3672 - acc: 0.8675\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3661 - acc: 0.8699\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3638 - acc: 0.8693\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3638 - acc: 0.8691\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3620 - acc: 0.8697\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 187us/step - loss: 0.3616 - acc: 0.8717\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 186us/step - loss: 0.3636 - acc: 0.8707\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3624 - acc: 0.8703\n",
            "24636/24636 [==============================] - 1s 56us/step\n",
            "24635/24635 [==============================] - 1s 38us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 232us/step - loss: 0.4531 - acc: 0.8343\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3976 - acc: 0.8500\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3855 - acc: 0.8636\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 184us/step - loss: 0.3785 - acc: 0.8650\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3734 - acc: 0.8667\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3713 - acc: 0.8718\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3707 - acc: 0.8714\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3693 - acc: 0.8737\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3645 - acc: 0.8749\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3629 - acc: 0.8757\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3597 - acc: 0.8790\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3621 - acc: 0.8770\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3563 - acc: 0.8785\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3580 - acc: 0.8783\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3596 - acc: 0.8779\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3576 - acc: 0.8803\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3568 - acc: 0.8808\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 186us/step - loss: 0.3575 - acc: 0.8809\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3552 - acc: 0.8814\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3568 - acc: 0.8788\n",
            "24635/24635 [==============================] - 1s 56us/step\n",
            "24636/24636 [==============================] - 1s 39us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 6s 235us/step - loss: 0.4410 - acc: 0.8497\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3794 - acc: 0.8669\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3780 - acc: 0.8693\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3775 - acc: 0.8699\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3736 - acc: 0.8713\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3748 - acc: 0.8717\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3672 - acc: 0.8743\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3671 - acc: 0.8742\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3644 - acc: 0.8718\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3644 - acc: 0.8732\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3628 - acc: 0.8731\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3635 - acc: 0.8733\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3647 - acc: 0.8716\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3637 - acc: 0.8731\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3618 - acc: 0.8731\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3649 - acc: 0.8725\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3638 - acc: 0.8751\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3659 - acc: 0.8727\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3643 - acc: 0.8740\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3620 - acc: 0.8733\n",
            "24636/24636 [==============================] - 1s 57us/step\n",
            "24635/24635 [==============================] - 1s 40us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 239us/step - loss: 0.4655 - acc: 0.8421\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 187us/step - loss: 0.3918 - acc: 0.8615\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3801 - acc: 0.8686\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3682 - acc: 0.8749\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3634 - acc: 0.8751\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3591 - acc: 0.8777\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3561 - acc: 0.8799\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3567 - acc: 0.8781\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3543 - acc: 0.8798\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3516 - acc: 0.8799\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3516 - acc: 0.8806\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3546 - acc: 0.8793\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3510 - acc: 0.8812\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3535 - acc: 0.8822\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3520 - acc: 0.8810\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3499 - acc: 0.8804\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3509 - acc: 0.8802\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3494 - acc: 0.8829\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3466 - acc: 0.8825\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3485 - acc: 0.8808\n",
            "24635/24635 [==============================] - 1s 60us/step\n",
            "24636/24636 [==============================] - 1s 41us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 6s 241us/step - loss: 0.4143 - acc: 0.8337\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3558 - acc: 0.8569\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3445 - acc: 0.8618\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3369 - acc: 0.8662\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3330 - acc: 0.8676\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3284 - acc: 0.8696\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3292 - acc: 0.8709\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3269 - acc: 0.8701\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3253 - acc: 0.8721\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3228 - acc: 0.8740\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3213 - acc: 0.8740\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3220 - acc: 0.8743\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3220 - acc: 0.8742\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3202 - acc: 0.8767\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3191 - acc: 0.8763\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3177 - acc: 0.8742\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3174 - acc: 0.8760\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3198 - acc: 0.8764\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3208 - acc: 0.8744\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3201 - acc: 0.8760\n",
            "24636/24636 [==============================] - 2s 63us/step\n",
            "24635/24635 [==============================] - 1s 42us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 244us/step - loss: 0.4006 - acc: 0.8366\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3570 - acc: 0.8541\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3444 - acc: 0.8619\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3385 - acc: 0.8667\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3321 - acc: 0.8680\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3291 - acc: 0.8660\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3251 - acc: 0.8718\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3213 - acc: 0.8744\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3217 - acc: 0.8752\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3226 - acc: 0.8777\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3187 - acc: 0.8765\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3140 - acc: 0.8776\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3149 - acc: 0.8801\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3139 - acc: 0.8773\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3163 - acc: 0.8792\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3132 - acc: 0.8780\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3116 - acc: 0.8792\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3105 - acc: 0.8805\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3112 - acc: 0.8815\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3104 - acc: 0.8801\n",
            "24635/24635 [==============================] - 2s 61us/step\n",
            "24636/24636 [==============================] - 1s 41us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 6s 245us/step - loss: 0.4250 - acc: 0.8257\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3585 - acc: 0.8536\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3508 - acc: 0.8598\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3471 - acc: 0.8606\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3431 - acc: 0.8628\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3406 - acc: 0.8669\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3384 - acc: 0.8647\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3340 - acc: 0.8677\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3321 - acc: 0.8678\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3305 - acc: 0.8675\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3291 - acc: 0.8701\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3273 - acc: 0.8685\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3269 - acc: 0.8710\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3251 - acc: 0.8714\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3257 - acc: 0.8699\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3272 - acc: 0.8701\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 188us/step - loss: 0.3275 - acc: 0.8717\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3237 - acc: 0.8710\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3218 - acc: 0.8708\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3232 - acc: 0.8704\n",
            "24636/24636 [==============================] - 2s 64us/step\n",
            "24635/24635 [==============================] - 1s 43us/step\n",
            "learning rate  0.003\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 256us/step - loss: 0.4171 - acc: 0.8363\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3526 - acc: 0.8604\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3464 - acc: 0.8614\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3394 - acc: 0.8654\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3374 - acc: 0.8683\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3350 - acc: 0.8681\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3317 - acc: 0.8699\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 188us/step - loss: 0.3288 - acc: 0.8709\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3274 - acc: 0.8705\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3255 - acc: 0.8728\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3251 - acc: 0.8723\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3242 - acc: 0.8717\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3231 - acc: 0.8734\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3221 - acc: 0.8717\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3273 - acc: 0.8729\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3213 - acc: 0.8744\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3226 - acc: 0.8747\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3225 - acc: 0.8753\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3193 - acc: 0.8736\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3192 - acc: 0.8745\n",
            "24635/24635 [==============================] - 2s 66us/step\n",
            "24636/24636 [==============================] - 1s 42us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 6s 256us/step - loss: 0.3933 - acc: 0.8375\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3660 - acc: 0.8516\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3584 - acc: 0.8564\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3572 - acc: 0.8557\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3528 - acc: 0.8564\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3505 - acc: 0.8597\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3526 - acc: 0.8612\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3474 - acc: 0.8631\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3478 - acc: 0.8605\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3438 - acc: 0.8637\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3451 - acc: 0.8621\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3441 - acc: 0.8641\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 189us/step - loss: 0.3428 - acc: 0.8646\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3413 - acc: 0.8623\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3416 - acc: 0.8637\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3432 - acc: 0.8637\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3466 - acc: 0.8649\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3418 - acc: 0.8641\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3401 - acc: 0.8651\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3410 - acc: 0.8654\n",
            "24636/24636 [==============================] - 2s 69us/step\n",
            "24635/24635 [==============================] - 1s 43us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 254us/step - loss: 0.3847 - acc: 0.8440\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3491 - acc: 0.8585\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3435 - acc: 0.8618\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 190us/step - loss: 0.3369 - acc: 0.8645\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3351 - acc: 0.8683\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3332 - acc: 0.8689\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3326 - acc: 0.8689\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3314 - acc: 0.8682\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3295 - acc: 0.8703\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3268 - acc: 0.8704\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 191us/step - loss: 0.3274 - acc: 0.8696\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3273 - acc: 0.8701\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3282 - acc: 0.8710\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3263 - acc: 0.8724\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3245 - acc: 0.8708\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3237 - acc: 0.8721\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 189us/step - loss: 0.3273 - acc: 0.8711\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3217 - acc: 0.8740\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3228 - acc: 0.8725\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3217 - acc: 0.8722\n",
            "24635/24635 [==============================] - 2s 71us/step\n",
            "24636/24636 [==============================] - 1s 44us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 7s 264us/step - loss: 0.4282 - acc: 0.8273\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3634 - acc: 0.8553\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3571 - acc: 0.8600\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.3503 - acc: 0.8630\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3533 - acc: 0.8615\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3451 - acc: 0.8623\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3433 - acc: 0.8649\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 190us/step - loss: 0.3394 - acc: 0.8650\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3400 - acc: 0.8666\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3360 - acc: 0.8641\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3356 - acc: 0.8682\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3364 - acc: 0.8664\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3362 - acc: 0.8678\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3384 - acc: 0.8691\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3311 - acc: 0.8682\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3308 - acc: 0.8669\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3324 - acc: 0.8671\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 192us/step - loss: 0.3346 - acc: 0.8673\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3358 - acc: 0.8685\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3339 - acc: 0.8692\n",
            "24636/24636 [==============================] - 2s 72us/step\n",
            "24635/24635 [==============================] - 1s 44us/step\n",
            "learning rate  0.006\n",
            "activation softplus\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 6s 263us/step - loss: 0.3829 - acc: 0.8474\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3546 - acc: 0.8593\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3465 - acc: 0.8652\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3441 - acc: 0.8670\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3386 - acc: 0.8696\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3403 - acc: 0.8714\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3357 - acc: 0.8709\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3331 - acc: 0.8734\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3296 - acc: 0.8740\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3284 - acc: 0.8742\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3275 - acc: 0.8746\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3256 - acc: 0.8751\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3258 - acc: 0.8745\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3246 - acc: 0.8757\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3261 - acc: 0.8782\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 193us/step - loss: 0.3226 - acc: 0.8753\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3264 - acc: 0.8767\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3236 - acc: 0.8765\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3241 - acc: 0.8761\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3219 - acc: 0.8788\n",
            "24635/24635 [==============================] - 2s 74us/step\n",
            "24636/24636 [==============================] - 1s 45us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 7s 269us/step - loss: 0.4574 - acc: 0.8352\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3563 - acc: 0.8691\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3410 - acc: 0.8798\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3371 - acc: 0.8801\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3347 - acc: 0.8794\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 199us/step - loss: 0.3264 - acc: 0.8842\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3253 - acc: 0.8846\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3191 - acc: 0.8843\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3191 - acc: 0.8840\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3200 - acc: 0.8859\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3178 - acc: 0.8854\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3210 - acc: 0.8849\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3126 - acc: 0.8869\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3159 - acc: 0.8873\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3201 - acc: 0.8840\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3174 - acc: 0.8848\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3198 - acc: 0.8845\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3155 - acc: 0.8873\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3164 - acc: 0.8862\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3197 - acc: 0.8851\n",
            "24636/24636 [==============================] - 2s 75us/step\n",
            "24635/24635 [==============================] - 1s 46us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 7s 269us/step - loss: 0.4697 - acc: 0.8274\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3666 - acc: 0.8640\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3404 - acc: 0.8763\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3295 - acc: 0.8816\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3249 - acc: 0.8847\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3225 - acc: 0.8843\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 192us/step - loss: 0.3193 - acc: 0.8851\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3192 - acc: 0.8850\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3167 - acc: 0.8860\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3188 - acc: 0.8854\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3156 - acc: 0.8855\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3214 - acc: 0.8835\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3172 - acc: 0.8855\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3144 - acc: 0.8855\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3161 - acc: 0.8835\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3135 - acc: 0.8845\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3159 - acc: 0.8861\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3152 - acc: 0.8857\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3129 - acc: 0.8840\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3143 - acc: 0.8846\n",
            "24635/24635 [==============================] - 2s 75us/step\n",
            "24636/24636 [==============================] - 1s 46us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 7s 276us/step - loss: 0.4744 - acc: 0.8439\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3520 - acc: 0.8723\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3384 - acc: 0.8788\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 199us/step - loss: 0.3310 - acc: 0.8806\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3257 - acc: 0.8835\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3242 - acc: 0.8832\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3235 - acc: 0.8854\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3212 - acc: 0.8846\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3242 - acc: 0.8856\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3215 - acc: 0.8863\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3189 - acc: 0.8847\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3172 - acc: 0.8859\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 193us/step - loss: 0.3192 - acc: 0.8846\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3205 - acc: 0.8845\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3188 - acc: 0.8851\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3208 - acc: 0.8857\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3203 - acc: 0.8852\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3188 - acc: 0.8841\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3178 - acc: 0.8856\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3171 - acc: 0.8855\n",
            "24636/24636 [==============================] - 2s 78us/step\n",
            "24635/24635 [==============================] - 1s 46us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 7s 274us/step - loss: 0.4729 - acc: 0.8509\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3577 - acc: 0.8741\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3429 - acc: 0.8804\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3362 - acc: 0.8817\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3315 - acc: 0.8843\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3270 - acc: 0.8852\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3285 - acc: 0.8846\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3301 - acc: 0.8836\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3286 - acc: 0.8859\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3280 - acc: 0.8844\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3226 - acc: 0.8864\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3268 - acc: 0.8847\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3295 - acc: 0.8857\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3249 - acc: 0.8839\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3250 - acc: 0.8850\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3267 - acc: 0.8848\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3246 - acc: 0.8877\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3232 - acc: 0.8851\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 194us/step - loss: 0.3217 - acc: 0.8853\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3232 - acc: 0.8865\n",
            "24635/24635 [==============================] - 2s 79us/step\n",
            "24636/24636 [==============================] - 1s 47us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 7s 275us/step - loss: 0.4194 - acc: 0.8571\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3510 - acc: 0.8764\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3481 - acc: 0.8788\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3511 - acc: 0.8783\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3479 - acc: 0.8803\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 204us/step - loss: 0.3419 - acc: 0.8816\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3448 - acc: 0.8822\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3377 - acc: 0.8831\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 191us/step - loss: 0.3426 - acc: 0.8788\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 199us/step - loss: 0.3401 - acc: 0.8818\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3389 - acc: 0.8835\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 199us/step - loss: 0.3415 - acc: 0.8801\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3415 - acc: 0.8830\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3406 - acc: 0.8821\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3408 - acc: 0.8826\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3364 - acc: 0.8816\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3365 - acc: 0.8835\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3378 - acc: 0.8825\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3390 - acc: 0.8793\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3386 - acc: 0.8832\n",
            "24636/24636 [==============================] - 2s 81us/step\n",
            "24635/24635 [==============================] - 1s 48us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 7s 282us/step - loss: 0.4541 - acc: 0.8256\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3849 - acc: 0.8584\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3722 - acc: 0.8622\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3567 - acc: 0.8735\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3434 - acc: 0.8822\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3391 - acc: 0.8814\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3442 - acc: 0.8830\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3393 - acc: 0.8822\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3398 - acc: 0.8828\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3329 - acc: 0.8853\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3343 - acc: 0.8826\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3329 - acc: 0.8849\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3334 - acc: 0.8828\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3397 - acc: 0.8832\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3324 - acc: 0.8836\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3314 - acc: 0.8850\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3339 - acc: 0.8860\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3357 - acc: 0.8845\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 195us/step - loss: 0.3347 - acc: 0.8843\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3358 - acc: 0.8823\n",
            "24635/24635 [==============================] - 2s 82us/step\n",
            "24636/24636 [==============================] - 1s 48us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 7s 285us/step - loss: 0.4450 - acc: 0.8455\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3764 - acc: 0.8659\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3705 - acc: 0.8722\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3596 - acc: 0.8755\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3547 - acc: 0.8779\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3571 - acc: 0.8767\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3561 - acc: 0.8771\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3465 - acc: 0.8818\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3466 - acc: 0.8791\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3500 - acc: 0.8814\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 194us/step - loss: 0.3487 - acc: 0.8804\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3422 - acc: 0.8820\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3403 - acc: 0.8826\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.3427 - acc: 0.8816\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3409 - acc: 0.8825\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3374 - acc: 0.8834\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3475 - acc: 0.8839\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3459 - acc: 0.8835\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3383 - acc: 0.8828\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3467 - acc: 0.8822\n",
            "24636/24636 [==============================] - 2s 85us/step\n",
            "24635/24635 [==============================] - 1s 49us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 7s 292us/step - loss: 0.4239 - acc: 0.8499\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3534 - acc: 0.8769\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3459 - acc: 0.8815\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3382 - acc: 0.8815\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3387 - acc: 0.8834\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3375 - acc: 0.8832\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3375 - acc: 0.8844\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3385 - acc: 0.8817\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3391 - acc: 0.8826\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3407 - acc: 0.8822\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3393 - acc: 0.8818\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3404 - acc: 0.8827\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3392 - acc: 0.8834\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3375 - acc: 0.8819\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3343 - acc: 0.8837\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3366 - acc: 0.8837\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 196us/step - loss: 0.3381 - acc: 0.8852\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3332 - acc: 0.8861\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3371 - acc: 0.8831\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3348 - acc: 0.8831\n",
            "24635/24635 [==============================] - 2s 87us/step\n",
            "24636/24636 [==============================] - 1s 50us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 7s 292us/step - loss: 0.4005 - acc: 0.8485\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3257 - acc: 0.8720\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3160 - acc: 0.8751\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3130 - acc: 0.8786\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3078 - acc: 0.8821\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3023 - acc: 0.8830\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.2992 - acc: 0.8846\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.2984 - acc: 0.8850\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.2980 - acc: 0.8836\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.2953 - acc: 0.8850\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.2963 - acc: 0.8853\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.2972 - acc: 0.8847\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.2943 - acc: 0.8854\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.2918 - acc: 0.8869\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.2925 - acc: 0.8856\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.2978 - acc: 0.8856\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.2948 - acc: 0.8853\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.2934 - acc: 0.8836\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.2934 - acc: 0.8833\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 195us/step - loss: 0.2912 - acc: 0.8860\n",
            "24636/24636 [==============================] - 2s 87us/step\n",
            "24635/24635 [==============================] - 1s 49us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 7s 300us/step - loss: 0.3972 - acc: 0.8354\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3301 - acc: 0.8701\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3161 - acc: 0.8774\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3097 - acc: 0.8794\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3025 - acc: 0.8829\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3012 - acc: 0.8844\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.2995 - acc: 0.8828\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.2992 - acc: 0.8852\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.3000 - acc: 0.8837\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.2974 - acc: 0.8845\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.2944 - acc: 0.8850\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.2941 - acc: 0.8863\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.2931 - acc: 0.8859\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.2904 - acc: 0.8873\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.2904 - acc: 0.8868\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.2909 - acc: 0.8860\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.2915 - acc: 0.8856\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 198us/step - loss: 0.2920 - acc: 0.8863\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.2910 - acc: 0.8861\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.2907 - acc: 0.8863\n",
            "24635/24635 [==============================] - 2s 91us/step\n",
            "24636/24636 [==============================] - 1s 50us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 7s 298us/step - loss: 0.4005 - acc: 0.8456\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 196us/step - loss: 0.3269 - acc: 0.8721\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.3174 - acc: 0.8764\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3117 - acc: 0.8777\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3109 - acc: 0.8832\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3089 - acc: 0.8808\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3048 - acc: 0.8813\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3024 - acc: 0.8811\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.2994 - acc: 0.8828\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.2984 - acc: 0.8831\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.2969 - acc: 0.8834\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.2993 - acc: 0.8823\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 199us/step - loss: 0.2968 - acc: 0.8846\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.2949 - acc: 0.8847\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.2958 - acc: 0.8836\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.2960 - acc: 0.8838\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.2952 - acc: 0.8863\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.2939 - acc: 0.8844\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 197us/step - loss: 0.2916 - acc: 0.8865\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.2936 - acc: 0.8860\n",
            "24636/24636 [==============================] - 2s 92us/step\n",
            "24635/24635 [==============================] - 1s 51us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 8s 305us/step - loss: 0.3982 - acc: 0.8444\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3273 - acc: 0.8719\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3166 - acc: 0.8777\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3120 - acc: 0.8792\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3110 - acc: 0.8807\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 197us/step - loss: 0.3082 - acc: 0.8803\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3073 - acc: 0.8813\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3052 - acc: 0.8837\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3058 - acc: 0.8827\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3012 - acc: 0.8846\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.2996 - acc: 0.8837\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.2976 - acc: 0.8846\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.2959 - acc: 0.8858\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.2982 - acc: 0.8843\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.2974 - acc: 0.8844\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.2939 - acc: 0.8848\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.2957 - acc: 0.8837\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.2939 - acc: 0.8853\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.2919 - acc: 0.8855\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.2940 - acc: 0.8833\n",
            "24635/24635 [==============================] - 2s 94us/step\n",
            "24636/24636 [==============================] - 1s 52us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 8s 305us/step - loss: 0.3816 - acc: 0.8522\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.3348 - acc: 0.8690\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3260 - acc: 0.8735\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3187 - acc: 0.8784\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3196 - acc: 0.8758\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 199us/step - loss: 0.3208 - acc: 0.8763\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 198us/step - loss: 0.3177 - acc: 0.8753\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3185 - acc: 0.8786\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.3152 - acc: 0.8788\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3136 - acc: 0.8792\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3138 - acc: 0.8787\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.3131 - acc: 0.8793\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3148 - acc: 0.8799\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.3124 - acc: 0.8790\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3109 - acc: 0.8799\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3092 - acc: 0.8815\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3124 - acc: 0.8799\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.3126 - acc: 0.8794\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 200us/step - loss: 0.3136 - acc: 0.8792\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 199us/step - loss: 0.3148 - acc: 0.8795\n",
            "24636/24636 [==============================] - 2s 95us/step\n",
            "24635/24635 [==============================] - 1s 52us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 8s 308us/step - loss: 0.3756 - acc: 0.8510\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3318 - acc: 0.8701\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3217 - acc: 0.8746\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3181 - acc: 0.8782\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3163 - acc: 0.8796\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3163 - acc: 0.8797\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3122 - acc: 0.8804\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3126 - acc: 0.8788\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3122 - acc: 0.8811\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3111 - acc: 0.8800\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3117 - acc: 0.8802\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 199us/step - loss: 0.3104 - acc: 0.8798\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3091 - acc: 0.8820\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3187 - acc: 0.8824\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3105 - acc: 0.8790\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3098 - acc: 0.8804\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3131 - acc: 0.8828\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3101 - acc: 0.8792\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3048 - acc: 0.8834\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 200us/step - loss: 0.3062 - acc: 0.8835\n",
            "24635/24635 [==============================] - 2s 97us/step\n",
            "24636/24636 [==============================] - 1s 52us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 8s 310us/step - loss: 0.3814 - acc: 0.8464\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3392 - acc: 0.8656\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3316 - acc: 0.8700\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3289 - acc: 0.8713\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 204us/step - loss: 0.3269 - acc: 0.8734\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3227 - acc: 0.8772\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3211 - acc: 0.8750\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3189 - acc: 0.8793\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3171 - acc: 0.8778\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3172 - acc: 0.8788\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3164 - acc: 0.8772\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 204us/step - loss: 0.3173 - acc: 0.8787\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 201us/step - loss: 0.3187 - acc: 0.8773\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3165 - acc: 0.8788\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3195 - acc: 0.8771\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3117 - acc: 0.8802\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 202us/step - loss: 0.3181 - acc: 0.8762\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3166 - acc: 0.8804\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3148 - acc: 0.8786\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 204us/step - loss: 0.3144 - acc: 0.8796\n",
            "24636/24636 [==============================] - 2s 100us/step\n",
            "24635/24635 [==============================] - 1s 53us/step\n",
            "learning rate  0.006\n",
            "activation relu\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 8s 318us/step - loss: 0.3652 - acc: 0.8566\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3313 - acc: 0.8728\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3228 - acc: 0.8768\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3202 - acc: 0.8773\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3163 - acc: 0.8783\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3155 - acc: 0.8796\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3096 - acc: 0.8811\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3111 - acc: 0.8792\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3111 - acc: 0.8804\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3111 - acc: 0.8803\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3088 - acc: 0.8809\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3114 - acc: 0.8803\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3095 - acc: 0.8801\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 201us/step - loss: 0.3120 - acc: 0.8791\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3107 - acc: 0.8829\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 202us/step - loss: 0.3089 - acc: 0.8809\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3136 - acc: 0.8809\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3083 - acc: 0.8814\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3096 - acc: 0.8798\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3091 - acc: 0.8806\n",
            "24635/24635 [==============================] - 3s 103us/step\n",
            "24636/24636 [==============================] - 1s 54us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 8s 323us/step - loss: 0.4620 - acc: 0.8345\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3571 - acc: 0.8655\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3461 - acc: 0.8732\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3448 - acc: 0.8740\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 204us/step - loss: 0.3439 - acc: 0.8751\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3420 - acc: 0.8761\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3399 - acc: 0.8793\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3366 - acc: 0.8824\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 204us/step - loss: 0.3375 - acc: 0.8825\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3337 - acc: 0.8821\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3371 - acc: 0.8822\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3327 - acc: 0.8844\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 204us/step - loss: 0.3317 - acc: 0.8813\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3321 - acc: 0.8819\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3317 - acc: 0.8842\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3283 - acc: 0.8837\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3236 - acc: 0.8840\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3278 - acc: 0.8836\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 203us/step - loss: 0.3244 - acc: 0.8833\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3286 - acc: 0.8846\n",
            "24636/24636 [==============================] - 3s 103us/step\n",
            "24635/24635 [==============================] - 1s 55us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 8s 325us/step - loss: 0.4436 - acc: 0.8437\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3494 - acc: 0.8699\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3460 - acc: 0.8719\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3415 - acc: 0.8750\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3402 - acc: 0.8776\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3390 - acc: 0.8772\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3338 - acc: 0.8784\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3342 - acc: 0.8792\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3353 - acc: 0.8780\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3324 - acc: 0.8800\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3342 - acc: 0.8776\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3322 - acc: 0.8810\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3343 - acc: 0.8785\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3334 - acc: 0.8791\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3328 - acc: 0.8794\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3326 - acc: 0.8793\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3308 - acc: 0.8803\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3299 - acc: 0.8799\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3306 - acc: 0.8794\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 203us/step - loss: 0.3284 - acc: 0.8796\n",
            "24635/24635 [==============================] - 3s 106us/step\n",
            "24636/24636 [==============================] - 1s 55us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 8s 333us/step - loss: 0.4901 - acc: 0.8243\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3780 - acc: 0.8603\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3573 - acc: 0.8719\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3432 - acc: 0.8774\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3422 - acc: 0.8790\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3378 - acc: 0.8812\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3318 - acc: 0.8820\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3294 - acc: 0.8835\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3290 - acc: 0.8841\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3275 - acc: 0.8841\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3273 - acc: 0.8860\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3242 - acc: 0.8846\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3215 - acc: 0.8867\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3247 - acc: 0.8883\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3208 - acc: 0.8865\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3202 - acc: 0.8854\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3211 - acc: 0.8872\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3223 - acc: 0.8862\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3236 - acc: 0.8861\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3248 - acc: 0.8857\n",
            "24636/24636 [==============================] - 3s 108us/step\n",
            "24635/24635 [==============================] - 1s 57us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 8s 333us/step - loss: 0.4776 - acc: 0.8480\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3629 - acc: 0.8698\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3553 - acc: 0.8756\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3471 - acc: 0.8789\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3428 - acc: 0.8807\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3435 - acc: 0.8809\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3336 - acc: 0.8825\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3367 - acc: 0.8833\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3319 - acc: 0.8836\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 204us/step - loss: 0.3345 - acc: 0.8816\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3347 - acc: 0.8837\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3336 - acc: 0.8832\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3304 - acc: 0.8842\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3299 - acc: 0.8858\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3291 - acc: 0.8840\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3294 - acc: 0.8844\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3276 - acc: 0.8863\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3281 - acc: 0.8853\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3281 - acc: 0.8851\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3295 - acc: 0.8840\n",
            "24635/24635 [==============================] - 3s 110us/step\n",
            "24636/24636 [==============================] - 1s 57us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 8s 335us/step - loss: 0.4371 - acc: 0.8443\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3655 - acc: 0.8697\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3594 - acc: 0.8742\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3537 - acc: 0.8789\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3519 - acc: 0.8772\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3488 - acc: 0.8800\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3475 - acc: 0.8807\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3430 - acc: 0.8823\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3439 - acc: 0.8810\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3423 - acc: 0.8803\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3454 - acc: 0.8820\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3441 - acc: 0.8814\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3431 - acc: 0.8819\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3394 - acc: 0.8811\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3409 - acc: 0.8817\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3383 - acc: 0.8833\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3399 - acc: 0.8820\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3391 - acc: 0.8823\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3485 - acc: 0.8804\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 205us/step - loss: 0.3416 - acc: 0.8831\n",
            "24636/24636 [==============================] - 3s 112us/step\n",
            "24635/24635 [==============================] - 1s 57us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 8s 342us/step - loss: 0.4256 - acc: 0.8533\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3723 - acc: 0.8718\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 205us/step - loss: 0.3549 - acc: 0.8786\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3474 - acc: 0.8810\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3433 - acc: 0.8833\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3430 - acc: 0.8816\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3374 - acc: 0.8814\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3466 - acc: 0.8825\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3384 - acc: 0.8825\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3421 - acc: 0.8826\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3407 - acc: 0.8821\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3375 - acc: 0.8825\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3391 - acc: 0.8840\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3356 - acc: 0.8846\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3370 - acc: 0.8834\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3371 - acc: 0.8818\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3440 - acc: 0.8813\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3393 - acc: 0.8845\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3341 - acc: 0.8842\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3388 - acc: 0.8840\n",
            "24635/24635 [==============================] - 3s 114us/step\n",
            "24636/24636 [==============================] - 1s 58us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 8s 343us/step - loss: 0.4402 - acc: 0.8472\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3746 - acc: 0.8712\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3671 - acc: 0.8762\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3598 - acc: 0.8785\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 206us/step - loss: 0.3563 - acc: 0.8791\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3550 - acc: 0.8805\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3503 - acc: 0.8817\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3486 - acc: 0.8798\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3500 - acc: 0.8819\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3534 - acc: 0.8798\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3469 - acc: 0.8809\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3474 - acc: 0.8805\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3473 - acc: 0.8800\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3495 - acc: 0.8827\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3485 - acc: 0.8809\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3417 - acc: 0.8818\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3525 - acc: 0.8807\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3408 - acc: 0.8824\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3435 - acc: 0.8825\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3438 - acc: 0.8807\n",
            "24636/24636 [==============================] - 3s 115us/step\n",
            "24635/24635 [==============================] - 1s 57us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 9s 349us/step - loss: 0.4416 - acc: 0.8467\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3696 - acc: 0.8719\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3624 - acc: 0.8745\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3562 - acc: 0.8777\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3530 - acc: 0.8790\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3530 - acc: 0.8790\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3491 - acc: 0.8774\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3514 - acc: 0.8797\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3426 - acc: 0.8840\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3471 - acc: 0.8804\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3481 - acc: 0.8811\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3436 - acc: 0.8818\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3417 - acc: 0.8826\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3515 - acc: 0.8806\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3484 - acc: 0.8827\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3476 - acc: 0.8812\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3455 - acc: 0.8809\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3441 - acc: 0.8833\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3387 - acc: 0.8837\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3398 - acc: 0.8834\n",
            "24635/24635 [==============================] - 3s 119us/step\n",
            "24636/24636 [==============================] - 1s 59us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 9s 350us/step - loss: 0.4065 - acc: 0.8349\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3421 - acc: 0.8628\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3282 - acc: 0.8702\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3216 - acc: 0.8738\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3190 - acc: 0.8729\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3167 - acc: 0.8747\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3163 - acc: 0.8777\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3148 - acc: 0.8777\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3133 - acc: 0.8785\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3124 - acc: 0.8785\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3116 - acc: 0.8795\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3113 - acc: 0.8799\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3111 - acc: 0.8792\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3106 - acc: 0.8793\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3111 - acc: 0.8801\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3095 - acc: 0.8808\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3090 - acc: 0.8779\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3110 - acc: 0.8806\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3094 - acc: 0.8804\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3084 - acc: 0.8808\n",
            "24636/24636 [==============================] - 3s 120us/step\n",
            "24635/24635 [==============================] - 1s 58us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 9s 351us/step - loss: 0.4033 - acc: 0.8325\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3343 - acc: 0.8695\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3208 - acc: 0.8763\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3178 - acc: 0.8771\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3130 - acc: 0.8800\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3098 - acc: 0.8806\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3077 - acc: 0.8804\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3057 - acc: 0.8831\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3055 - acc: 0.8823\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3046 - acc: 0.8837\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3039 - acc: 0.8835\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3048 - acc: 0.8823\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3038 - acc: 0.8824\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3018 - acc: 0.8832\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3017 - acc: 0.8832\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3010 - acc: 0.8826\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.2996 - acc: 0.8845\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3002 - acc: 0.8844\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.2993 - acc: 0.8838\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.2997 - acc: 0.8829\n",
            "24635/24635 [==============================] - 3s 122us/step\n",
            "24636/24636 [==============================] - 1s 58us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 9s 364us/step - loss: 0.4031 - acc: 0.8291\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3496 - acc: 0.8592\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3313 - acc: 0.8710\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3218 - acc: 0.8745\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3200 - acc: 0.8761\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3146 - acc: 0.8804\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3133 - acc: 0.8770\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3117 - acc: 0.8791\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3151 - acc: 0.8776\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3112 - acc: 0.8809\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3117 - acc: 0.8791\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3074 - acc: 0.8816\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3104 - acc: 0.8813\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3066 - acc: 0.8824\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3062 - acc: 0.8818\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3054 - acc: 0.8817\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3084 - acc: 0.8816\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3046 - acc: 0.8824\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3065 - acc: 0.8816\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3039 - acc: 0.8828\n",
            "24636/24636 [==============================] - 3s 123us/step\n",
            "24635/24635 [==============================] - 1s 60us/step\n",
            "learning rate  0.003\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 9s 362us/step - loss: 0.5236 - acc: 0.8277\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3668 - acc: 0.8634\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3402 - acc: 0.8701\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3295 - acc: 0.8745\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3247 - acc: 0.8781\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3224 - acc: 0.8777\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3214 - acc: 0.8780\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3187 - acc: 0.8801\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3183 - acc: 0.8805\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3171 - acc: 0.8808\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3166 - acc: 0.8816\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3151 - acc: 0.8811\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3155 - acc: 0.8814\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3163 - acc: 0.8810\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3157 - acc: 0.8815\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3159 - acc: 0.8797\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3140 - acc: 0.8825\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3154 - acc: 0.8813\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3142 - acc: 0.8832\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3128 - acc: 0.8818\n",
            "24635/24635 [==============================] - 3s 127us/step\n",
            "24636/24636 [==============================] - 1s 60us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 9s 371us/step - loss: 0.3747 - acc: 0.8524\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3390 - acc: 0.8666\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3358 - acc: 0.8690\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3292 - acc: 0.8727\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3284 - acc: 0.8751\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3239 - acc: 0.8757\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3239 - acc: 0.8750\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3221 - acc: 0.8756\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3211 - acc: 0.8752\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3204 - acc: 0.8762\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3205 - acc: 0.8782\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3194 - acc: 0.8777\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3191 - acc: 0.8773\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3188 - acc: 0.8781\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3170 - acc: 0.8799\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3209 - acc: 0.8799\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3195 - acc: 0.8781\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3181 - acc: 0.8754\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3176 - acc: 0.8779\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3149 - acc: 0.8790\n",
            "24636/24636 [==============================] - 3s 129us/step\n",
            "24635/24635 [==============================] - 1s 61us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 9s 373us/step - loss: 0.3733 - acc: 0.8527\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3350 - acc: 0.8706\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3272 - acc: 0.8744\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3250 - acc: 0.8753\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3208 - acc: 0.8755\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3188 - acc: 0.8764\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3218 - acc: 0.8759\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3154 - acc: 0.8798\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3161 - acc: 0.8781\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3166 - acc: 0.8781\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3158 - acc: 0.8792\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3154 - acc: 0.8795\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3120 - acc: 0.8801\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3164 - acc: 0.8785\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3155 - acc: 0.8783\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 206us/step - loss: 0.3139 - acc: 0.8803\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3130 - acc: 0.8803\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3137 - acc: 0.8815\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 207us/step - loss: 0.3122 - acc: 0.8809\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3161 - acc: 0.8766\n",
            "24635/24635 [==============================] - 3s 132us/step\n",
            "24636/24636 [==============================] - 1s 61us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 9s 372us/step - loss: 0.3832 - acc: 0.8441\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3504 - acc: 0.8616\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3482 - acc: 0.8666\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3375 - acc: 0.8703\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3353 - acc: 0.8697\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3326 - acc: 0.8728\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3315 - acc: 0.8722\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3303 - acc: 0.8728\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3296 - acc: 0.8728\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3272 - acc: 0.8749\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 208us/step - loss: 0.3262 - acc: 0.8745\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3264 - acc: 0.8751\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3245 - acc: 0.8777\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3263 - acc: 0.8766\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3233 - acc: 0.8772\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3245 - acc: 0.8759\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 207us/step - loss: 0.3261 - acc: 0.8763\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3235 - acc: 0.8771\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3219 - acc: 0.8779\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3222 - acc: 0.8773\n",
            "24636/24636 [==============================] - 3s 132us/step\n",
            "24635/24635 [==============================] - 2s 64us/step\n",
            "learning rate  0.006\n",
            "activation tanh\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 9s 377us/step - loss: 0.3898 - acc: 0.8449\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3433 - acc: 0.8655\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3335 - acc: 0.8700\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3287 - acc: 0.8734\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3257 - acc: 0.8756\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3205 - acc: 0.8779\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3198 - acc: 0.8781\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3195 - acc: 0.8770\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3199 - acc: 0.8769\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3164 - acc: 0.8789\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3188 - acc: 0.8767\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3185 - acc: 0.8785\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3171 - acc: 0.8782\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3172 - acc: 0.8806\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3153 - acc: 0.8787\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3144 - acc: 0.8805\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3147 - acc: 0.8796\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3148 - acc: 0.8798\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3149 - acc: 0.8805\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 208us/step - loss: 0.3179 - acc: 0.8793\n",
            "24635/24635 [==============================] - 3s 133us/step\n",
            "24636/24636 [==============================] - 2s 61us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 10s 386us/step - loss: 0.4917 - acc: 0.8071\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3869 - acc: 0.8453\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3705 - acc: 0.8587\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3636 - acc: 0.8647\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3612 - acc: 0.8669\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3579 - acc: 0.8699\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3569 - acc: 0.8721\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3566 - acc: 0.8714\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 209us/step - loss: 0.3560 - acc: 0.8708\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3547 - acc: 0.8723\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3554 - acc: 0.8702\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 210us/step - loss: 0.3531 - acc: 0.8718\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3517 - acc: 0.8718\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3560 - acc: 0.8709\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3522 - acc: 0.8725\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3518 - acc: 0.8716\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3530 - acc: 0.8698\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3531 - acc: 0.8723\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3551 - acc: 0.8712\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3506 - acc: 0.8720\n",
            "24636/24636 [==============================] - 3s 138us/step\n",
            "24635/24635 [==============================] - 2s 63us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 10s 391us/step - loss: 0.5164 - acc: 0.7811\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3884 - acc: 0.8463\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3724 - acc: 0.8585\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3653 - acc: 0.8645\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3620 - acc: 0.8666\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3587 - acc: 0.8691\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3583 - acc: 0.8700\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3581 - acc: 0.8710\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3570 - acc: 0.8717\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3561 - acc: 0.8727\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3567 - acc: 0.8719\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 209us/step - loss: 0.3559 - acc: 0.8729\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3535 - acc: 0.8726\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 210us/step - loss: 0.3558 - acc: 0.8710\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3535 - acc: 0.8720\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3520 - acc: 0.8731\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3512 - acc: 0.8732\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3523 - acc: 0.8738\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3511 - acc: 0.8745\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3521 - acc: 0.8729\n",
            "24635/24635 [==============================] - 3s 141us/step\n",
            "24636/24636 [==============================] - 2s 62us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 10s 392us/step - loss: 0.4929 - acc: 0.8216\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.4017 - acc: 0.8435\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3838 - acc: 0.8542\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3729 - acc: 0.8598\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3691 - acc: 0.8642\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3672 - acc: 0.8666\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3665 - acc: 0.8660\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3642 - acc: 0.8670\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3638 - acc: 0.8687\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3644 - acc: 0.8687\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3620 - acc: 0.8688\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3613 - acc: 0.8689\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3592 - acc: 0.8698\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3601 - acc: 0.8714\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3575 - acc: 0.8720\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3605 - acc: 0.8697\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3583 - acc: 0.8715\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3584 - acc: 0.8712\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3571 - acc: 0.8721\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 211us/step - loss: 0.3571 - acc: 0.8710\n",
            "24636/24636 [==============================] - 3s 141us/step\n",
            "24635/24635 [==============================] - 2s 65us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 10s 397us/step - loss: 0.4873 - acc: 0.8278\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3974 - acc: 0.8460\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3830 - acc: 0.8528\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3761 - acc: 0.8598\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3739 - acc: 0.8609\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3712 - acc: 0.8626\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3683 - acc: 0.8626\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3678 - acc: 0.8659\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3669 - acc: 0.8652\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3646 - acc: 0.8678\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3637 - acc: 0.8692\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3640 - acc: 0.8692\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3617 - acc: 0.8686\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3624 - acc: 0.8711\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3606 - acc: 0.8692\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3608 - acc: 0.8697\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3582 - acc: 0.8716\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3572 - acc: 0.8725\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3587 - acc: 0.8723\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3572 - acc: 0.8723\n",
            "24635/24635 [==============================] - 4s 145us/step\n",
            "24636/24636 [==============================] - 2s 65us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 10s 402us/step - loss: 0.4584 - acc: 0.8255\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3912 - acc: 0.8527\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3830 - acc: 0.8570\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3813 - acc: 0.8604\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3765 - acc: 0.8637\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3780 - acc: 0.8636\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3753 - acc: 0.8663\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3723 - acc: 0.8680\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3712 - acc: 0.8686\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3690 - acc: 0.8700\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3693 - acc: 0.8696\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3691 - acc: 0.8706\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3676 - acc: 0.8700\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3685 - acc: 0.8712\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3650 - acc: 0.8722\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3641 - acc: 0.8720\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3647 - acc: 0.8721\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3653 - acc: 0.8712\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3652 - acc: 0.8716\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3621 - acc: 0.8714\n",
            "24636/24636 [==============================] - 4s 143us/step\n",
            "24635/24635 [==============================] - 2s 64us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 10s 399us/step - loss: 0.4533 - acc: 0.8311\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.4011 - acc: 0.8477\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 211us/step - loss: 0.3899 - acc: 0.8553\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3825 - acc: 0.8609\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3799 - acc: 0.8637\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3772 - acc: 0.8657\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3724 - acc: 0.8665\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3736 - acc: 0.8698\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3765 - acc: 0.8674\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3734 - acc: 0.8699\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3736 - acc: 0.8694\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3763 - acc: 0.8694\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3742 - acc: 0.8710\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3728 - acc: 0.8699\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3731 - acc: 0.8694\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3703 - acc: 0.8709\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3702 - acc: 0.8704\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 212us/step - loss: 0.3729 - acc: 0.8719\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3705 - acc: 0.8707\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3702 - acc: 0.8717\n",
            "24635/24635 [==============================] - 4s 147us/step\n",
            "24636/24636 [==============================] - 2s 65us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 10s 406us/step - loss: 0.4533 - acc: 0.8375\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.4021 - acc: 0.8510\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3914 - acc: 0.8611\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3854 - acc: 0.8625\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3846 - acc: 0.8626\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3740 - acc: 0.8652\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3694 - acc: 0.8679\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3713 - acc: 0.8680\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3705 - acc: 0.8708\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3682 - acc: 0.8719\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3661 - acc: 0.8717\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3662 - acc: 0.8729\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3636 - acc: 0.8721\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3636 - acc: 0.8734\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 212us/step - loss: 0.3644 - acc: 0.8704\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3605 - acc: 0.8734\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3634 - acc: 0.8730\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3656 - acc: 0.8728\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3614 - acc: 0.8726\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3607 - acc: 0.8734\n",
            "24636/24636 [==============================] - 4s 148us/step\n",
            "24635/24635 [==============================] - 2s 66us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l1\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 10s 411us/step - loss: 0.4506 - acc: 0.8347\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3939 - acc: 0.8490\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3871 - acc: 0.8562\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3797 - acc: 0.8595\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3784 - acc: 0.8616\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3721 - acc: 0.8652\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3723 - acc: 0.8658\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3696 - acc: 0.8677\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3690 - acc: 0.8681\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3665 - acc: 0.8697\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3652 - acc: 0.8691\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3646 - acc: 0.8712\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3670 - acc: 0.8720\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3639 - acc: 0.8697\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3620 - acc: 0.8706\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3631 - acc: 0.8730\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3621 - acc: 0.8720\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3630 - acc: 0.8709\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3614 - acc: 0.8713\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3634 - acc: 0.8715\n",
            "24635/24635 [==============================] - 4s 149us/step\n",
            "24636/24636 [==============================] - 2s 66us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 10s 415us/step - loss: 0.4080 - acc: 0.8345\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 213us/step - loss: 0.3683 - acc: 0.8507\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3613 - acc: 0.8549\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3557 - acc: 0.8560\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3519 - acc: 0.8584\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3486 - acc: 0.8620\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3470 - acc: 0.8616\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3441 - acc: 0.8634\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3421 - acc: 0.8642\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3401 - acc: 0.8642\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3378 - acc: 0.8645\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3377 - acc: 0.8665\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3368 - acc: 0.8660\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3345 - acc: 0.8672\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3346 - acc: 0.8668\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3333 - acc: 0.8665\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3331 - acc: 0.8676\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3322 - acc: 0.8682\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3332 - acc: 0.8666\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3334 - acc: 0.8684\n",
            "24636/24636 [==============================] - 4s 155us/step\n",
            "24635/24635 [==============================] - 2s 67us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 10s 418us/step - loss: 0.4209 - acc: 0.8194\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3719 - acc: 0.8468\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3650 - acc: 0.8515\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3618 - acc: 0.8519\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3591 - acc: 0.8537\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3573 - acc: 0.8544\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3571 - acc: 0.8549\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3550 - acc: 0.8546\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3543 - acc: 0.8564\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3528 - acc: 0.8564\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3508 - acc: 0.8569\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3490 - acc: 0.8597\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3489 - acc: 0.8576\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3464 - acc: 0.8609\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3466 - acc: 0.8604\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3437 - acc: 0.8630\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3423 - acc: 0.8627\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3414 - acc: 0.8644\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3404 - acc: 0.8640\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 213us/step - loss: 0.3392 - acc: 0.8665\n",
            "24635/24635 [==============================] - 4s 157us/step\n",
            "24636/24636 [==============================] - 2s 67us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 10s 424us/step - loss: 0.4218 - acc: 0.8240\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3857 - acc: 0.8356\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3805 - acc: 0.8379\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3741 - acc: 0.8449\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3685 - acc: 0.8476\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 214us/step - loss: 0.3668 - acc: 0.8511\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3614 - acc: 0.8510\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3586 - acc: 0.8532\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3574 - acc: 0.8547\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3555 - acc: 0.8559\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3566 - acc: 0.8551\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3524 - acc: 0.8542\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3516 - acc: 0.8560\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3515 - acc: 0.8558\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3511 - acc: 0.8574\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 215us/step - loss: 0.3521 - acc: 0.8589\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3492 - acc: 0.8572\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3465 - acc: 0.8584\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3462 - acc: 0.8586\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3451 - acc: 0.8606\n",
            "24636/24636 [==============================] - 4s 158us/step\n",
            "24635/24635 [==============================] - 2s 69us/step\n",
            "learning rate  0.003\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 11s 427us/step - loss: 0.4214 - acc: 0.8176\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3685 - acc: 0.8484\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3611 - acc: 0.8525\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3565 - acc: 0.8564\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3518 - acc: 0.8590\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3466 - acc: 0.8611\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 215us/step - loss: 0.3417 - acc: 0.8655\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 214us/step - loss: 0.3378 - acc: 0.8665\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.3359 - acc: 0.8692\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 221us/step - loss: 0.3339 - acc: 0.8703\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 222us/step - loss: 0.3320 - acc: 0.8708\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3311 - acc: 0.8723\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3303 - acc: 0.8722\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3296 - acc: 0.8721\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3267 - acc: 0.8733\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3270 - acc: 0.8732\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 220us/step - loss: 0.3239 - acc: 0.8742\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3242 - acc: 0.8747\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3236 - acc: 0.8753\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3236 - acc: 0.8739\n",
            "24635/24635 [==============================] - 4s 163us/step\n",
            "24636/24636 [==============================] - 2s 69us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 11s 434us/step - loss: 0.4179 - acc: 0.8195\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3803 - acc: 0.8414\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3744 - acc: 0.8460\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3696 - acc: 0.8492\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3661 - acc: 0.8494\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3599 - acc: 0.8531\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3574 - acc: 0.8528\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3544 - acc: 0.8552\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3526 - acc: 0.8584\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3537 - acc: 0.8593\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3485 - acc: 0.8626\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3484 - acc: 0.8624\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3480 - acc: 0.8624\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3454 - acc: 0.8655\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3432 - acc: 0.8626\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3438 - acc: 0.8660\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3465 - acc: 0.8655\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3438 - acc: 0.8649\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3412 - acc: 0.8679\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 216us/step - loss: 0.3430 - acc: 0.8664\n",
            "24636/24636 [==============================] - 4s 162us/step\n",
            "24635/24635 [==============================] - 2s 68us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 11s 438us/step - loss: 0.3990 - acc: 0.8344\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3751 - acc: 0.8473\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3675 - acc: 0.8507\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3640 - acc: 0.8528\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3588 - acc: 0.8544\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3571 - acc: 0.8559\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3542 - acc: 0.8576\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3476 - acc: 0.8621\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 220us/step - loss: 0.3456 - acc: 0.8645\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3436 - acc: 0.8652\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3427 - acc: 0.8669\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3394 - acc: 0.8669\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3381 - acc: 0.8712\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 220us/step - loss: 0.3365 - acc: 0.8694\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3343 - acc: 0.8717\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3331 - acc: 0.8729\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3326 - acc: 0.8732\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3310 - acc: 0.8722\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 217us/step - loss: 0.3278 - acc: 0.8745\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 216us/step - loss: 0.3263 - acc: 0.8758\n",
            "24635/24635 [==============================] - 4s 165us/step\n",
            "24636/24636 [==============================] - 2s 69us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 11s 441us/step - loss: 0.4028 - acc: 0.8295\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3813 - acc: 0.8401\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3735 - acc: 0.8454\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3715 - acc: 0.8493\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3662 - acc: 0.8503\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3614 - acc: 0.8540\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3617 - acc: 0.8549\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3556 - acc: 0.8572\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 220us/step - loss: 0.3558 - acc: 0.8602\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3515 - acc: 0.8632\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3475 - acc: 0.8649\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3496 - acc: 0.8634\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 5s 217us/step - loss: 0.3430 - acc: 0.8674\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3444 - acc: 0.8662\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3424 - acc: 0.8668\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3401 - acc: 0.8674\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 5s 219us/step - loss: 0.3411 - acc: 0.8673\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3398 - acc: 0.8673\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3403 - acc: 0.8696\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 5s 218us/step - loss: 0.3367 - acc: 0.8688\n",
            "24636/24636 [==============================] - 4s 168us/step\n",
            "24635/24635 [==============================] - 2s 70us/step\n",
            "learning rate  0.006\n",
            "activation sigmoid\n",
            "neurons in 1st layer 14\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 11s 444us/step - loss: 0.4067 - acc: 0.8303\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 222us/step - loss: 0.3754 - acc: 0.8436\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3708 - acc: 0.8469\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 5s 220us/step - loss: 0.3671 - acc: 0.8503\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3640 - acc: 0.8512\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3603 - acc: 0.8522\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 5s 220us/step - loss: 0.3581 - acc: 0.8540\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 5s 222us/step - loss: 0.3553 - acc: 0.8561\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 5s 221us/step - loss: 0.3511 - acc: 0.8571\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 5s 220us/step - loss: 0.3479 - acc: 0.8594\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3471 - acc: 0.8601\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 220us/step - loss: 0.3442 - acc: 0.8644\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3453 - acc: 0.8646\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3435 - acc: 0.8635\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3424 - acc: 0.8651\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3405 - acc: 0.8667\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 5s 218us/step - loss: 0.3392 - acc: 0.8658\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3405 - acc: 0.8685\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 5s 221us/step - loss: 0.3391 - acc: 0.8688\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 5s 219us/step - loss: 0.3375 - acc: 0.8688\n",
            "24635/24635 [==============================] - 4s 170us/step\n",
            "24636/24636 [==============================] - 2s 70us/step\n",
            "learning rate  0.003\n",
            "activation relu\n",
            "neurons in 1st layer 10\n",
            "kernel regularizer l2\n",
            "Epoch 1/20\n",
            "49271/49271 [==============================] - 17s 336us/step - loss: 0.4902 - acc: 0.8190\n",
            "Epoch 2/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3449 - acc: 0.8680\n",
            "Epoch 3/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3308 - acc: 0.8743\n",
            "Epoch 4/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3265 - acc: 0.8764\n",
            "Epoch 5/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3248 - acc: 0.8768\n",
            "Epoch 6/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3225 - acc: 0.8777\n",
            "Epoch 7/20\n",
            "49271/49271 [==============================] - 11s 224us/step - loss: 0.3224 - acc: 0.8788\n",
            "Epoch 8/20\n",
            "49271/49271 [==============================] - 11s 223us/step - loss: 0.3198 - acc: 0.8782\n",
            "Epoch 9/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3179 - acc: 0.8795\n",
            "Epoch 10/20\n",
            "49271/49271 [==============================] - 11s 220us/step - loss: 0.3135 - acc: 0.8801\n",
            "Epoch 11/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3095 - acc: 0.8799\n",
            "Epoch 12/20\n",
            "49271/49271 [==============================] - 11s 220us/step - loss: 0.3058 - acc: 0.8815\n",
            "Epoch 13/20\n",
            "49271/49271 [==============================] - 11s 220us/step - loss: 0.3084 - acc: 0.8815\n",
            "Epoch 14/20\n",
            "49271/49271 [==============================] - 11s 218us/step - loss: 0.3088 - acc: 0.8810\n",
            "Epoch 15/20\n",
            "49271/49271 [==============================] - 11s 218us/step - loss: 0.3094 - acc: 0.8820\n",
            "Epoch 16/20\n",
            "49271/49271 [==============================] - 11s 221us/step - loss: 0.3051 - acc: 0.8817\n",
            "Epoch 17/20\n",
            "49271/49271 [==============================] - 11s 219us/step - loss: 0.3049 - acc: 0.8814\n",
            "Epoch 18/20\n",
            "49271/49271 [==============================] - 11s 218us/step - loss: 0.3036 - acc: 0.8816\n",
            "Epoch 19/20\n",
            "49271/49271 [==============================] - 11s 219us/step - loss: 0.3042 - acc: 0.8823\n",
            "Epoch 20/20\n",
            "49271/49271 [==============================] - 11s 218us/step - loss: 0.3039 - acc: 0.8823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_pAHdqwkdoaa",
        "colab_type": "code",
        "outputId": "aaa3b09d-3eb4-4f74-cd1d-890e27560d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.666964 using {'activation': 'relu', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.505569 (0.045240) with: {'activation': 'softmax', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.545521 (0.038753) with: {'activation': 'softmax', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.547626 (0.079408) with: {'activation': 'softmax', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.564513 (0.014979) with: {'activation': 'softmax', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.617526 (0.006506) with: {'activation': 'softmax', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.590407 (0.033024) with: {'activation': 'softmax', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.617365 (0.010372) with: {'activation': 'softmax', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.296956 (0.296962) with: {'activation': 'softmax', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.559352 (0.021515) with: {'activation': 'softplus', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.620666 (0.006347) with: {'activation': 'softplus', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.601905 (0.039441) with: {'activation': 'softplus', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.599371 (0.038093) with: {'activation': 'softplus', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.609082 (0.022204) with: {'activation': 'softplus', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.581045 (0.039063) with: {'activation': 'softplus', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.552558 (0.022066) with: {'activation': 'softplus', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.607346 (0.028483) with: {'activation': 'softplus', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.663701 (0.001780) with: {'activation': 'relu', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.658266 (0.011277) with: {'activation': 'relu', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.628640 (0.016107) with: {'activation': 'relu', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.637093 (0.023561) with: {'activation': 'relu', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.666964 (0.009656) with: {'activation': 'relu', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.640617 (0.024243) with: {'activation': 'relu', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.630432 (0.009290) with: {'activation': 'relu', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.634366 (0.021874) with: {'activation': 'relu', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.612114 (0.027928) with: {'activation': 'tanh', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.643344 (0.021903) with: {'activation': 'tanh', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.653118 (0.005629) with: {'activation': 'tanh', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.626608 (0.031809) with: {'activation': 'tanh', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.663856 (0.000948) with: {'activation': 'tanh', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.627128 (0.022798) with: {'activation': 'tanh', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.607443 (0.036207) with: {'activation': 'tanh', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.646998 (0.004616) with: {'activation': 'tanh', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.569815 (0.061620) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.554537 (0.031909) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l1', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.570059 (0.007443) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.593401 (0.054097) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l1', 'learn_rate': 0.006, 'neurons': 14}\n",
            "0.598756 (0.003050) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 10}\n",
            "0.536053 (0.050602) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l2', 'learn_rate': 0.003, 'neurons': 14}\n",
            "0.515119 (0.097485) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 10}\n",
            "0.600350 (0.030762) with: {'activation': 'sigmoid', 'kernel_regularizer': 'l2', 'learn_rate': 0.006, 'neurons': 14}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ytfg2uVldoXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(init_mode):\n",
        "    \n",
        "#     # create model\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(4, input_dim=15, kernel_initializer=init_mode, activation='tanh'))\n",
        "#     model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
        "    \n",
        "#     # Compile model\n",
        "#     sgd=optimizers.SGD(0.001)\n",
        "#     model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    print(\"init_mode\", init_mode)\n",
        "  \n",
        "    # create model\n",
        "    adam = optimizers.Adam(lr=0.003)\n",
        "    \n",
        "    \n",
        "    model_MLP = Sequential()\n",
        "    model_MLP.add(Dense(10,input_dim=input_dim, kernel_initializer=init_mode, activation='relu',kernel_regularizer='l2'))\n",
        "    model_MLP.add(Dense(4, activation='relu', kernel_initializer=init_mode, kernel_constraint=maxnorm(3)))\n",
        "    model_MLP.add(Dense(1,activation='sigmoid'))\n",
        "    \n",
        "    # compile model\n",
        "    model_MLP.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model_MLP\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yB_e70SDdoVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = KerasClassifier(build_fn=create_model, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M1P6bEpEdoSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7f5ca0ff-28d2-4d45-de4b-0af93712ab4a"
      },
      "cell_type": "code",
      "source": [
        "init_mode = ['uniform', 'lecun_uniform']\n",
        "# kernel_regularizer = ['l1', 'l2']\n",
        "# activation = ['softmax', 'softplus', 'relu', 'tanh', 'sigmoid']\n",
        "batch_size = [32, 64]\n",
        "# learn_rate = [0.003, 0.006]\n",
        "epochs=[20,30]\n",
        "# neurons = [10, 14]\n",
        "# batch_size=[32]\n",
        "param_grid = dict(\n",
        "    init_mode=init_mode,\n",
        "#                   kernel_regularizer = kernel_regularizer, \n",
        "                  batch_size=batch_size,\n",
        "#                   activation = activation, \n",
        "                  epochs = epochs\n",
        "#                   learn_rate = learn_rate, neurons=neurons\n",
        "                 )\n",
        "param_grid"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': [32, 64],\n",
              " 'epochs': [20, 30],\n",
              " 'init_mode': ['uniform', 'lecun_uniform']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "0XBamLcNdoOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14952
        },
        "outputId": "cbac1a09-6ca3-4a6a-87cc-942b83c10bb5"
      },
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, scoring='f1')\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init_mode uniform\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 12s 469us/step - loss: 0.3922 - acc: 0.8287\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3584 - acc: 0.8536\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3515 - acc: 0.8569\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3489 - acc: 0.8622\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3482 - acc: 0.8624\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3452 - acc: 0.8631\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3430 - acc: 0.8622\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3435 - acc: 0.8634\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3423 - acc: 0.8636\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3400 - acc: 0.8640\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3386 - acc: 0.8639\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3385 - acc: 0.8623\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3416 - acc: 0.8636\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3468 - acc: 0.8630\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3376 - acc: 0.8658\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3390 - acc: 0.8633\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3370 - acc: 0.8637\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3402 - acc: 0.8636\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3386 - acc: 0.8632\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3359 - acc: 0.8633\n",
            "24636/24636 [==============================] - 4s 175us/step\n",
            "24635/24635 [==============================] - 2s 73us/step\n",
            "init_mode uniform\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 11s 465us/step - loss: 0.4074 - acc: 0.8200\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.3615 - acc: 0.8526\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.3529 - acc: 0.8542\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.3459 - acc: 0.8590\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.3318 - acc: 0.8690\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.3189 - acc: 0.8771\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.3118 - acc: 0.8786\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.3078 - acc: 0.8816\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.3052 - acc: 0.8817\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.3033 - acc: 0.8841\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.3048 - acc: 0.8830\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.3018 - acc: 0.8833\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 6s 223us/step - loss: 0.3021 - acc: 0.8860\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.2985 - acc: 0.8871\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.2994 - acc: 0.8842\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2991 - acc: 0.8857\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2991 - acc: 0.8845\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.2994 - acc: 0.8846\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2978 - acc: 0.8857\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2979 - acc: 0.8857\n",
            "24635/24635 [==============================] - 4s 178us/step\n",
            "24636/24636 [==============================] - 2s 75us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 12s 468us/step - loss: 0.3966 - acc: 0.8388\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3294 - acc: 0.8695\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3186 - acc: 0.8740\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3150 - acc: 0.8773\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 5s 222us/step - loss: 0.3093 - acc: 0.8792\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3105 - acc: 0.8787\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3080 - acc: 0.8810\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3055 - acc: 0.8820\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3048 - acc: 0.8839\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3037 - acc: 0.8828\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 6s 223us/step - loss: 0.3020 - acc: 0.8810\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3007 - acc: 0.8841\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.2990 - acc: 0.8835\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3008 - acc: 0.8826\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 5s 221us/step - loss: 0.2987 - acc: 0.8842\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.2987 - acc: 0.8855\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 6s 223us/step - loss: 0.3003 - acc: 0.8841\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.2999 - acc: 0.8857\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3008 - acc: 0.8841\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.2970 - acc: 0.8844\n",
            "24636/24636 [==============================] - 4s 181us/step\n",
            "24635/24635 [==============================] - 2s 74us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 12s 470us/step - loss: 0.3798 - acc: 0.8521\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.3269 - acc: 0.8710\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.3142 - acc: 0.8785\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.3088 - acc: 0.8817\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.3028 - acc: 0.8820\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.3035 - acc: 0.8824\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2998 - acc: 0.8844\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2983 - acc: 0.8840\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2969 - acc: 0.8843\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2959 - acc: 0.8855\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2947 - acc: 0.8853\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.2952 - acc: 0.8850\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.2953 - acc: 0.8847\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2944 - acc: 0.8847\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2982 - acc: 0.8845\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.3007 - acc: 0.8861\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2917 - acc: 0.8863\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2918 - acc: 0.8850\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2934 - acc: 0.8847\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2931 - acc: 0.8839\n",
            "24635/24635 [==============================] - 4s 182us/step\n",
            "24636/24636 [==============================] - 2s 74us/step\n",
            "init_mode uniform\n",
            "Epoch 1/30\n",
            "24635/24635 [==============================] - 12s 479us/step - loss: 0.3838 - acc: 0.8334\n",
            "Epoch 2/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3554 - acc: 0.8535\n",
            "Epoch 3/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3533 - acc: 0.8594\n",
            "Epoch 4/30\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3475 - acc: 0.8589\n",
            "Epoch 5/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3497 - acc: 0.8623\n",
            "Epoch 6/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3455 - acc: 0.8619\n",
            "Epoch 7/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3419 - acc: 0.8610\n",
            "Epoch 8/30\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3438 - acc: 0.8630\n",
            "Epoch 9/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3421 - acc: 0.8623\n",
            "Epoch 10/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3391 - acc: 0.8621\n",
            "Epoch 11/30\n",
            "24635/24635 [==============================] - 6s 223us/step - loss: 0.3381 - acc: 0.8639\n",
            "Epoch 12/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3419 - acc: 0.8634\n",
            "Epoch 13/30\n",
            "24635/24635 [==============================] - 6s 223us/step - loss: 0.3446 - acc: 0.8632\n",
            "Epoch 14/30\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3370 - acc: 0.8654\n",
            "Epoch 15/30\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3435 - acc: 0.8638\n",
            "Epoch 16/30\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3465 - acc: 0.8654\n",
            "Epoch 17/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3376 - acc: 0.8628\n",
            "Epoch 18/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3371 - acc: 0.8637\n",
            "Epoch 19/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3369 - acc: 0.8636\n",
            "Epoch 20/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3414 - acc: 0.8634\n",
            "Epoch 21/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3388 - acc: 0.8649\n",
            "Epoch 22/30\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3430 - acc: 0.8644\n",
            "Epoch 23/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3382 - acc: 0.8641\n",
            "Epoch 24/30\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3404 - acc: 0.8636\n",
            "Epoch 25/30\n",
            "24635/24635 [==============================] - 6s 224us/step - loss: 0.3394 - acc: 0.8641\n",
            "Epoch 26/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3392 - acc: 0.8634\n",
            "Epoch 27/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3373 - acc: 0.8643\n",
            "Epoch 28/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3349 - acc: 0.8641\n",
            "Epoch 29/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3349 - acc: 0.8641\n",
            "Epoch 30/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3354 - acc: 0.8631\n",
            "24636/24636 [==============================] - 5s 186us/step\n",
            "24635/24635 [==============================] - 2s 74us/step\n",
            "init_mode uniform\n",
            "Epoch 1/30\n",
            "24636/24636 [==============================] - 12s 482us/step - loss: 0.3816 - acc: 0.8349\n",
            "Epoch 2/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.3297 - acc: 0.8684\n",
            "Epoch 3/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.3171 - acc: 0.8766\n",
            "Epoch 4/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.3110 - acc: 0.8817\n",
            "Epoch 5/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.3046 - acc: 0.8835\n",
            "Epoch 6/30\n",
            "24636/24636 [==============================] - 6s 229us/step - loss: 0.3025 - acc: 0.8835\n",
            "Epoch 7/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.3009 - acc: 0.8857\n",
            "Epoch 8/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2981 - acc: 0.8856\n",
            "Epoch 9/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.3012 - acc: 0.8857\n",
            "Epoch 10/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2967 - acc: 0.8847\n",
            "Epoch 11/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2971 - acc: 0.8859\n",
            "Epoch 12/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2950 - acc: 0.8854\n",
            "Epoch 13/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2934 - acc: 0.8855\n",
            "Epoch 14/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2935 - acc: 0.8856\n",
            "Epoch 15/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2965 - acc: 0.8852\n",
            "Epoch 16/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2930 - acc: 0.8855\n",
            "Epoch 17/30\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2956 - acc: 0.8847\n",
            "Epoch 18/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2923 - acc: 0.8847\n",
            "Epoch 19/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2929 - acc: 0.8854\n",
            "Epoch 20/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2934 - acc: 0.8847\n",
            "Epoch 21/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2961 - acc: 0.8848\n",
            "Epoch 22/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2928 - acc: 0.8851\n",
            "Epoch 23/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2939 - acc: 0.8855\n",
            "Epoch 24/30\n",
            "24636/24636 [==============================] - 6s 224us/step - loss: 0.2948 - acc: 0.8851\n",
            "Epoch 25/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2927 - acc: 0.8863\n",
            "Epoch 26/30\n",
            "24636/24636 [==============================] - 5s 223us/step - loss: 0.2913 - acc: 0.8846\n",
            "Epoch 27/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2956 - acc: 0.8830\n",
            "Epoch 28/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2927 - acc: 0.8844\n",
            "Epoch 29/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2917 - acc: 0.8849\n",
            "Epoch 30/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2936 - acc: 0.8840\n",
            "24635/24635 [==============================] - 5s 187us/step\n",
            "24636/24636 [==============================] - 2s 76us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/30\n",
            "24635/24635 [==============================] - 12s 483us/step - loss: 0.5290 - acc: 0.8155\n",
            "Epoch 2/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3631 - acc: 0.8680\n",
            "Epoch 3/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3355 - acc: 0.8750\n",
            "Epoch 4/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3275 - acc: 0.8753\n",
            "Epoch 5/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3240 - acc: 0.8778\n",
            "Epoch 6/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3227 - acc: 0.8779\n",
            "Epoch 7/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3213 - acc: 0.8768\n",
            "Epoch 8/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3184 - acc: 0.8793\n",
            "Epoch 9/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3171 - acc: 0.8794\n",
            "Epoch 10/30\n",
            "24635/24635 [==============================] - 6s 228us/step - loss: 0.3128 - acc: 0.8813\n",
            "Epoch 11/30\n",
            "24635/24635 [==============================] - 5s 223us/step - loss: 0.3134 - acc: 0.8797\n",
            "Epoch 12/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3115 - acc: 0.8806\n",
            "Epoch 13/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3114 - acc: 0.8806\n",
            "Epoch 14/30\n",
            "24635/24635 [==============================] - 6s 225us/step - loss: 0.3117 - acc: 0.8805\n",
            "Epoch 15/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3105 - acc: 0.8790\n",
            "Epoch 16/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3092 - acc: 0.8812\n",
            "Epoch 17/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3105 - acc: 0.8794\n",
            "Epoch 18/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3109 - acc: 0.8784\n",
            "Epoch 19/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3087 - acc: 0.8818\n",
            "Epoch 20/30\n",
            "24635/24635 [==============================] - 6s 227us/step - loss: 0.3097 - acc: 0.8809\n",
            "Epoch 21/30\n",
            "24635/24635 [==============================] - 6s 228us/step - loss: 0.3104 - acc: 0.8806\n",
            "Epoch 22/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3090 - acc: 0.8796\n",
            "Epoch 23/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3078 - acc: 0.8805\n",
            "Epoch 24/30\n",
            "24635/24635 [==============================] - 6s 228us/step - loss: 0.3109 - acc: 0.8784\n",
            "Epoch 25/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3091 - acc: 0.8809\n",
            "Epoch 26/30\n",
            "24635/24635 [==============================] - 6s 228us/step - loss: 0.3093 - acc: 0.8794\n",
            "Epoch 27/30\n",
            "24635/24635 [==============================] - 6s 229us/step - loss: 0.3095 - acc: 0.8791\n",
            "Epoch 28/30\n",
            "24635/24635 [==============================] - 6s 228us/step - loss: 0.3082 - acc: 0.8796\n",
            "Epoch 29/30\n",
            "24635/24635 [==============================] - 6s 230us/step - loss: 0.3095 - acc: 0.8809\n",
            "Epoch 30/30\n",
            "24635/24635 [==============================] - 6s 226us/step - loss: 0.3095 - acc: 0.8798\n",
            "24636/24636 [==============================] - 5s 191us/step\n",
            "24635/24635 [==============================] - 2s 76us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/30\n",
            "24636/24636 [==============================] - 12s 490us/step - loss: 0.3757 - acc: 0.8522\n",
            "Epoch 2/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.3215 - acc: 0.8747\n",
            "Epoch 3/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.3130 - acc: 0.8784\n",
            "Epoch 4/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.3065 - acc: 0.8820\n",
            "Epoch 5/30\n",
            "24636/24636 [==============================] - 6s 229us/step - loss: 0.3029 - acc: 0.8849\n",
            "Epoch 6/30\n",
            "24636/24636 [==============================] - 6s 229us/step - loss: 0.3014 - acc: 0.8836\n",
            "Epoch 7/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2971 - acc: 0.8850\n",
            "Epoch 8/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2971 - acc: 0.8857\n",
            "Epoch 9/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2989 - acc: 0.8841\n",
            "Epoch 10/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2929 - acc: 0.8861\n",
            "Epoch 11/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2929 - acc: 0.8861\n",
            "Epoch 12/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2927 - acc: 0.8846\n",
            "Epoch 13/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2925 - acc: 0.8843\n",
            "Epoch 14/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2920 - acc: 0.8852\n",
            "Epoch 15/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2900 - acc: 0.8847\n",
            "Epoch 16/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2921 - acc: 0.8863\n",
            "Epoch 17/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2964 - acc: 0.8870\n",
            "Epoch 18/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2889 - acc: 0.8861\n",
            "Epoch 19/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2891 - acc: 0.8875\n",
            "Epoch 20/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2894 - acc: 0.8863\n",
            "Epoch 21/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2904 - acc: 0.8865\n",
            "Epoch 22/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2910 - acc: 0.8865\n",
            "Epoch 23/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2881 - acc: 0.8870\n",
            "Epoch 24/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2873 - acc: 0.8851\n",
            "Epoch 25/30\n",
            "24636/24636 [==============================] - 6s 225us/step - loss: 0.2911 - acc: 0.8868\n",
            "Epoch 26/30\n",
            "24636/24636 [==============================] - 6s 227us/step - loss: 0.2880 - acc: 0.8874\n",
            "Epoch 27/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2890 - acc: 0.8859\n",
            "Epoch 28/30\n",
            "24636/24636 [==============================] - 6s 226us/step - loss: 0.2878 - acc: 0.8890\n",
            "Epoch 29/30\n",
            "24636/24636 [==============================] - 6s 229us/step - loss: 0.2908 - acc: 0.8858\n",
            "Epoch 30/30\n",
            "24636/24636 [==============================] - 6s 228us/step - loss: 0.2883 - acc: 0.8857\n",
            "24635/24635 [==============================] - 5s 191us/step\n",
            "24636/24636 [==============================] - 2s 76us/step\n",
            "init_mode uniform\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 9s 378us/step - loss: 0.4157 - acc: 0.8124\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3633 - acc: 0.8481\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3558 - acc: 0.8550\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3494 - acc: 0.8580\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3494 - acc: 0.8601\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3427 - acc: 0.8634\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3276 - acc: 0.8697\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3133 - acc: 0.8757\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3051 - acc: 0.8813\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3017 - acc: 0.8824\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2984 - acc: 0.8834\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2996 - acc: 0.8853\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 3s 113us/step - loss: 0.2955 - acc: 0.8859\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2937 - acc: 0.8884\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2931 - acc: 0.8854\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2960 - acc: 0.8852\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2968 - acc: 0.8865\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2955 - acc: 0.8872\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2966 - acc: 0.8857\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2938 - acc: 0.8865\n",
            "24636/24636 [==============================] - 4s 156us/step\n",
            "24635/24635 [==============================] - 1s 38us/step\n",
            "init_mode uniform\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 9s 384us/step - loss: 0.3885 - acc: 0.8229\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 3s 113us/step - loss: 0.3377 - acc: 0.8658\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 3s 113us/step - loss: 0.3224 - acc: 0.8740\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 3s 114us/step - loss: 0.3121 - acc: 0.8796\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 3s 114us/step - loss: 0.3050 - acc: 0.8842\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.3031 - acc: 0.8841\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2983 - acc: 0.8854\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 3s 114us/step - loss: 0.2968 - acc: 0.8857\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 3s 114us/step - loss: 0.2935 - acc: 0.8872\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2899 - acc: 0.8864\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2921 - acc: 0.8864\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.3026 - acc: 0.8861\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2924 - acc: 0.8861\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2893 - acc: 0.8868\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2886 - acc: 0.8868\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 3s 113us/step - loss: 0.2877 - acc: 0.8855\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 3s 114us/step - loss: 0.2873 - acc: 0.8876\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2871 - acc: 0.8878\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2876 - acc: 0.8882\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2882 - acc: 0.8868\n",
            "24635/24635 [==============================] - 4s 159us/step\n",
            "24636/24636 [==============================] - 1s 38us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/20\n",
            "24635/24635 [==============================] - 9s 386us/step - loss: 0.4229 - acc: 0.8313\n",
            "Epoch 2/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3319 - acc: 0.8694\n",
            "Epoch 3/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3193 - acc: 0.8743\n",
            "Epoch 4/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3116 - acc: 0.8784\n",
            "Epoch 5/20\n",
            "24635/24635 [==============================] - 3s 113us/step - loss: 0.3085 - acc: 0.8816\n",
            "Epoch 6/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.3031 - acc: 0.8814\n",
            "Epoch 7/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3007 - acc: 0.8837\n",
            "Epoch 8/20\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3003 - acc: 0.8832\n",
            "Epoch 9/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2968 - acc: 0.8848\n",
            "Epoch 10/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2949 - acc: 0.8854\n",
            "Epoch 11/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2949 - acc: 0.8854\n",
            "Epoch 12/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2940 - acc: 0.8850\n",
            "Epoch 13/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2928 - acc: 0.8851\n",
            "Epoch 14/20\n",
            "24635/24635 [==============================] - 3s 113us/step - loss: 0.2939 - acc: 0.8850\n",
            "Epoch 15/20\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2911 - acc: 0.8859\n",
            "Epoch 16/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2924 - acc: 0.8851\n",
            "Epoch 17/20\n",
            "24635/24635 [==============================] - 3s 113us/step - loss: 0.2910 - acc: 0.8866\n",
            "Epoch 18/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2895 - acc: 0.8869\n",
            "Epoch 19/20\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2933 - acc: 0.8855\n",
            "Epoch 20/20\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2928 - acc: 0.8856\n",
            "24636/24636 [==============================] - 4s 159us/step\n",
            "24635/24635 [==============================] - 1s 39us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/20\n",
            "24636/24636 [==============================] - 10s 395us/step - loss: 0.4150 - acc: 0.8382\n",
            "Epoch 2/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.3296 - acc: 0.8689\n",
            "Epoch 3/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.3182 - acc: 0.8738\n",
            "Epoch 4/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.3089 - acc: 0.8803\n",
            "Epoch 5/20\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.3059 - acc: 0.8809\n",
            "Epoch 6/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.3004 - acc: 0.8838\n",
            "Epoch 7/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2997 - acc: 0.8833\n",
            "Epoch 8/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2966 - acc: 0.8857\n",
            "Epoch 9/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2940 - acc: 0.8854\n",
            "Epoch 10/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2949 - acc: 0.8846\n",
            "Epoch 11/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2934 - acc: 0.8871\n",
            "Epoch 12/20\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2925 - acc: 0.8863\n",
            "Epoch 13/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2917 - acc: 0.8872\n",
            "Epoch 14/20\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2908 - acc: 0.8861\n",
            "Epoch 15/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2895 - acc: 0.8868\n",
            "Epoch 16/20\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2896 - acc: 0.8869\n",
            "Epoch 17/20\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2909 - acc: 0.8861\n",
            "Epoch 18/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2895 - acc: 0.8872\n",
            "Epoch 19/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2863 - acc: 0.8884\n",
            "Epoch 20/20\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2871 - acc: 0.8883\n",
            "24635/24635 [==============================] - 4s 162us/step\n",
            "24636/24636 [==============================] - 1s 39us/step\n",
            "init_mode uniform\n",
            "Epoch 1/30\n",
            "24635/24635 [==============================] - 10s 397us/step - loss: 0.4312 - acc: 0.8117\n",
            "Epoch 2/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.3682 - acc: 0.8338\n",
            "Epoch 3/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3583 - acc: 0.8518\n",
            "Epoch 4/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3546 - acc: 0.8571\n",
            "Epoch 5/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3502 - acc: 0.8579\n",
            "Epoch 6/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3477 - acc: 0.8603\n",
            "Epoch 7/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3443 - acc: 0.8628\n",
            "Epoch 8/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3409 - acc: 0.8646\n",
            "Epoch 9/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3389 - acc: 0.8622\n",
            "Epoch 10/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3324 - acc: 0.8675\n",
            "Epoch 11/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3226 - acc: 0.8738\n",
            "Epoch 12/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3132 - acc: 0.8756\n",
            "Epoch 13/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3060 - acc: 0.8798\n",
            "Epoch 14/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3033 - acc: 0.8810\n",
            "Epoch 15/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3036 - acc: 0.8825\n",
            "Epoch 16/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3021 - acc: 0.8854\n",
            "Epoch 17/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2993 - acc: 0.8860\n",
            "Epoch 18/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2996 - acc: 0.8847\n",
            "Epoch 19/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2985 - acc: 0.8857\n",
            "Epoch 20/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2944 - acc: 0.8864\n",
            "Epoch 21/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2927 - acc: 0.8862\n",
            "Epoch 22/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2933 - acc: 0.8861\n",
            "Epoch 23/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.2917 - acc: 0.8858\n",
            "Epoch 24/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2927 - acc: 0.8859\n",
            "Epoch 25/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2913 - acc: 0.8849\n",
            "Epoch 26/30\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2951 - acc: 0.8859\n",
            "Epoch 27/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2907 - acc: 0.8861\n",
            "Epoch 28/30\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2888 - acc: 0.8876\n",
            "Epoch 29/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2879 - acc: 0.8864\n",
            "Epoch 30/30\n",
            "24635/24635 [==============================] - 3s 114us/step - loss: 0.2888 - acc: 0.8869\n",
            "24636/24636 [==============================] - 4s 163us/step\n",
            "24635/24635 [==============================] - 1s 40us/step\n",
            "init_mode uniform\n",
            "Epoch 1/30\n",
            "24636/24636 [==============================] - 10s 400us/step - loss: 0.4133 - acc: 0.8180\n",
            "Epoch 2/30\n",
            "24636/24636 [==============================] - 3s 114us/step - loss: 0.3600 - acc: 0.8433\n",
            "Epoch 3/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.3464 - acc: 0.8604\n",
            "Epoch 4/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.3283 - acc: 0.8720\n",
            "Epoch 5/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.3150 - acc: 0.8789\n",
            "Epoch 6/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.3063 - acc: 0.8829\n",
            "Epoch 7/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.3011 - acc: 0.8840\n",
            "Epoch 8/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2975 - acc: 0.8867\n",
            "Epoch 9/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2967 - acc: 0.8857\n",
            "Epoch 10/30\n",
            "24636/24636 [==============================] - 3s 118us/step - loss: 0.2956 - acc: 0.8866\n",
            "Epoch 11/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2947 - acc: 0.8882\n",
            "Epoch 12/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2930 - acc: 0.8868\n",
            "Epoch 13/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2943 - acc: 0.8882\n",
            "Epoch 14/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2945 - acc: 0.8867\n",
            "Epoch 15/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2937 - acc: 0.8868\n",
            "Epoch 16/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2940 - acc: 0.8878\n",
            "Epoch 17/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2927 - acc: 0.8865\n",
            "Epoch 18/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2921 - acc: 0.8881\n",
            "Epoch 19/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2918 - acc: 0.8873\n",
            "Epoch 20/30\n",
            "24636/24636 [==============================] - 3s 118us/step - loss: 0.2917 - acc: 0.8875\n",
            "Epoch 21/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2913 - acc: 0.8862\n",
            "Epoch 22/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2900 - acc: 0.8872\n",
            "Epoch 23/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2894 - acc: 0.8887\n",
            "Epoch 24/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2922 - acc: 0.8859\n",
            "Epoch 25/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2899 - acc: 0.8883\n",
            "Epoch 26/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2878 - acc: 0.8874\n",
            "Epoch 27/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2883 - acc: 0.8886\n",
            "Epoch 28/30\n",
            "24636/24636 [==============================] - 3s 118us/step - loss: 0.2897 - acc: 0.8876\n",
            "Epoch 29/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2906 - acc: 0.8864\n",
            "Epoch 30/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2922 - acc: 0.8870\n",
            "24635/24635 [==============================] - 4s 166us/step\n",
            "24636/24636 [==============================] - 1s 40us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/30\n",
            "24635/24635 [==============================] - 10s 404us/step - loss: 0.4295 - acc: 0.8257\n",
            "Epoch 2/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3408 - acc: 0.8661\n",
            "Epoch 3/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.3250 - acc: 0.8728\n",
            "Epoch 4/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3166 - acc: 0.8737\n",
            "Epoch 5/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3120 - acc: 0.8792\n",
            "Epoch 6/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3098 - acc: 0.8788\n",
            "Epoch 7/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3069 - acc: 0.8797\n",
            "Epoch 8/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3045 - acc: 0.8814\n",
            "Epoch 9/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.3031 - acc: 0.8817\n",
            "Epoch 10/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.3000 - acc: 0.8821\n",
            "Epoch 11/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2998 - acc: 0.8838\n",
            "Epoch 12/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.2975 - acc: 0.8845\n",
            "Epoch 13/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2969 - acc: 0.8832\n",
            "Epoch 14/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.2955 - acc: 0.8858\n",
            "Epoch 15/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2942 - acc: 0.8840\n",
            "Epoch 16/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2964 - acc: 0.8849\n",
            "Epoch 17/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.2959 - acc: 0.8849\n",
            "Epoch 18/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2918 - acc: 0.8857\n",
            "Epoch 19/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2927 - acc: 0.8834\n",
            "Epoch 20/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.2928 - acc: 0.8862\n",
            "Epoch 21/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2910 - acc: 0.8863\n",
            "Epoch 22/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2909 - acc: 0.8848\n",
            "Epoch 23/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.2901 - acc: 0.8851\n",
            "Epoch 24/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2906 - acc: 0.8854\n",
            "Epoch 25/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2899 - acc: 0.8869\n",
            "Epoch 26/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2909 - acc: 0.8835\n",
            "Epoch 27/30\n",
            "24635/24635 [==============================] - 3s 117us/step - loss: 0.2890 - acc: 0.8861\n",
            "Epoch 28/30\n",
            "24635/24635 [==============================] - 3s 118us/step - loss: 0.2903 - acc: 0.8867\n",
            "Epoch 29/30\n",
            "24635/24635 [==============================] - 3s 115us/step - loss: 0.2891 - acc: 0.8857\n",
            "Epoch 30/30\n",
            "24635/24635 [==============================] - 3s 116us/step - loss: 0.2921 - acc: 0.8838\n",
            "24636/24636 [==============================] - 4s 168us/step\n",
            "24635/24635 [==============================] - 1s 40us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/30\n",
            "24636/24636 [==============================] - 10s 409us/step - loss: 0.4102 - acc: 0.8379\n",
            "Epoch 2/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.3327 - acc: 0.8689\n",
            "Epoch 3/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.3198 - acc: 0.8736\n",
            "Epoch 4/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.3114 - acc: 0.8785\n",
            "Epoch 5/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.3068 - acc: 0.8816\n",
            "Epoch 6/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.3038 - acc: 0.8822\n",
            "Epoch 7/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2985 - acc: 0.8850\n",
            "Epoch 8/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2997 - acc: 0.8819\n",
            "Epoch 9/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2948 - acc: 0.8863\n",
            "Epoch 10/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2929 - acc: 0.8883\n",
            "Epoch 11/30\n",
            "24636/24636 [==============================] - 3s 118us/step - loss: 0.2902 - acc: 0.8878\n",
            "Epoch 12/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2884 - acc: 0.8873\n",
            "Epoch 13/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2912 - acc: 0.8878\n",
            "Epoch 14/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2893 - acc: 0.8859\n",
            "Epoch 15/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2981 - acc: 0.8873\n",
            "Epoch 16/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2874 - acc: 0.8878\n",
            "Epoch 17/30\n",
            "24636/24636 [==============================] - 3s 117us/step - loss: 0.2862 - acc: 0.8885\n",
            "Epoch 18/30\n",
            "24636/24636 [==============================] - 3s 115us/step - loss: 0.2861 - acc: 0.8856\n",
            "Epoch 19/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2864 - acc: 0.8866\n",
            "Epoch 20/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2861 - acc: 0.8867\n",
            "Epoch 21/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2853 - acc: 0.8874\n",
            "Epoch 22/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2858 - acc: 0.8873\n",
            "Epoch 23/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2848 - acc: 0.8890\n",
            "Epoch 24/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2834 - acc: 0.8878\n",
            "Epoch 25/30\n",
            "24636/24636 [==============================] - 3s 118us/step - loss: 0.2877 - acc: 0.8886\n",
            "Epoch 26/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2846 - acc: 0.8872\n",
            "Epoch 27/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2837 - acc: 0.8876\n",
            "Epoch 28/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2859 - acc: 0.8865\n",
            "Epoch 29/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2832 - acc: 0.8881\n",
            "Epoch 30/30\n",
            "24636/24636 [==============================] - 3s 116us/step - loss: 0.2834 - acc: 0.8878\n",
            "24635/24635 [==============================] - 4s 169us/step\n",
            "24636/24636 [==============================] - 1s 39us/step\n",
            "init_mode lecun_uniform\n",
            "Epoch 1/20\n",
            "49271/49271 [==============================] - 19s 380us/step - loss: 0.3597 - acc: 0.8575\n",
            "Epoch 2/20\n",
            "49271/49271 [==============================] - 11s 233us/step - loss: 0.3170 - acc: 0.8771\n",
            "Epoch 3/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.3119 - acc: 0.8796\n",
            "Epoch 4/20\n",
            "49271/49271 [==============================] - 11s 233us/step - loss: 0.3076 - acc: 0.8810\n",
            "Epoch 5/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.3056 - acc: 0.8829\n",
            "Epoch 6/20\n",
            "49271/49271 [==============================] - 11s 232us/step - loss: 0.3027 - acc: 0.8814\n",
            "Epoch 7/20\n",
            "49271/49271 [==============================] - 11s 232us/step - loss: 0.3029 - acc: 0.8822\n",
            "Epoch 8/20\n",
            "49271/49271 [==============================] - 11s 233us/step - loss: 0.3019 - acc: 0.8808\n",
            "Epoch 9/20\n",
            "49271/49271 [==============================] - 11s 231us/step - loss: 0.2981 - acc: 0.8830\n",
            "Epoch 10/20\n",
            "49271/49271 [==============================] - 11s 232us/step - loss: 0.2997 - acc: 0.8837\n",
            "Epoch 11/20\n",
            "49271/49271 [==============================] - 11s 231us/step - loss: 0.2966 - acc: 0.8827\n",
            "Epoch 12/20\n",
            "49271/49271 [==============================] - 11s 230us/step - loss: 0.2965 - acc: 0.8837\n",
            "Epoch 13/20\n",
            "49271/49271 [==============================] - 11s 232us/step - loss: 0.2969 - acc: 0.8836\n",
            "Epoch 14/20\n",
            "49271/49271 [==============================] - 11s 233us/step - loss: 0.2947 - acc: 0.8849\n",
            "Epoch 15/20\n",
            "49271/49271 [==============================] - 11s 231us/step - loss: 0.2947 - acc: 0.8850\n",
            "Epoch 16/20\n",
            "49271/49271 [==============================] - 11s 231us/step - loss: 0.2930 - acc: 0.8854\n",
            "Epoch 17/20\n",
            "49271/49271 [==============================] - 11s 231us/step - loss: 0.2957 - acc: 0.8840\n",
            "Epoch 18/20\n",
            "49271/49271 [==============================] - 11s 231us/step - loss: 0.2953 - acc: 0.8846\n",
            "Epoch 19/20\n",
            "49271/49271 [==============================] - 11s 230us/step - loss: 0.2922 - acc: 0.8859\n",
            "Epoch 20/20\n",
            "49271/49271 [==============================] - 11s 230us/step - loss: 0.2983 - acc: 0.8851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1aR-B2neK3pa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "5159c834-1bd0-4bd1-d83a-82b66122dc96"
      },
      "cell_type": "code",
      "source": [
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.681447 using {'batch_size': 32, 'epochs': 20, 'init_mode': 'lecun_uniform'}\n",
            "0.603531 (0.025665) with: {'batch_size': 32, 'epochs': 20, 'init_mode': 'uniform'}\n",
            "0.681447 (0.011319) with: {'batch_size': 32, 'epochs': 20, 'init_mode': 'lecun_uniform'}\n",
            "0.579996 (0.050080) with: {'batch_size': 32, 'epochs': 30, 'init_mode': 'uniform'}\n",
            "0.659840 (0.000426) with: {'batch_size': 32, 'epochs': 30, 'init_mode': 'lecun_uniform'}\n",
            "0.674095 (0.004834) with: {'batch_size': 64, 'epochs': 20, 'init_mode': 'uniform'}\n",
            "0.663128 (0.006818) with: {'batch_size': 64, 'epochs': 20, 'init_mode': 'lecun_uniform'}\n",
            "0.655220 (0.006957) with: {'batch_size': 64, 'epochs': 30, 'init_mode': 'uniform'}\n",
            "0.660640 (0.001376) with: {'batch_size': 64, 'epochs': 30, 'init_mode': 'lecun_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_ECXuOaJXcqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Running the final model after getting best parameters"
      ]
    },
    {
      "metadata": {
        "id": "fntoXvHiXcWI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "adam = optimizers.Adam(lr=0.003)\n",
        "    \n",
        "    \n",
        "model_MLP = Sequential()\n",
        "model_MLP.add(Dense(10,input_dim=input_dim, kernel_initializer='lecun_uniform', activation='relu',kernel_regularizer='l2'))\n",
        "model_MLP.add(Dense(4, activation='relu', kernel_initializer='lecun_uniform', kernel_constraint=maxnorm(3)))\n",
        "model_MLP.add(Dense(1,activation='sigmoid'))\n",
        "    \n",
        "# compile model\n",
        "model_MLP.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HY27T6y5K3k-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "0128e4d4-d320-4def-fe70-dbe38b89a7ac"
      },
      "cell_type": "code",
      "source": [
        "model_MLP.fit(X_train, y_train, epochs=20, batch_size=32)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49271/49271 [==============================] - 19s 388us/step - loss: 0.3578 - acc: 0.8555\n",
            "Epoch 2/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.3152 - acc: 0.8779\n",
            "Epoch 3/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.3079 - acc: 0.8801\n",
            "Epoch 4/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.3018 - acc: 0.8830\n",
            "Epoch 5/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.3009 - acc: 0.8838\n",
            "Epoch 6/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.2991 - acc: 0.8830\n",
            "Epoch 7/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.2983 - acc: 0.8841\n",
            "Epoch 8/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.2984 - acc: 0.8839\n",
            "Epoch 9/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.2949 - acc: 0.8852\n",
            "Epoch 10/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.2959 - acc: 0.8851\n",
            "Epoch 11/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.2938 - acc: 0.8848\n",
            "Epoch 12/20\n",
            "49271/49271 [==============================] - 12s 237us/step - loss: 0.2949 - acc: 0.8838\n",
            "Epoch 13/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.2939 - acc: 0.8854\n",
            "Epoch 14/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.2942 - acc: 0.8845\n",
            "Epoch 15/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.2942 - acc: 0.8849\n",
            "Epoch 16/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.2932 - acc: 0.8842\n",
            "Epoch 17/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.2926 - acc: 0.8864\n",
            "Epoch 18/20\n",
            "49271/49271 [==============================] - 12s 235us/step - loss: 0.2941 - acc: 0.8848\n",
            "Epoch 19/20\n",
            "49271/49271 [==============================] - 12s 234us/step - loss: 0.2918 - acc: 0.8849\n",
            "Epoch 20/20\n",
            "49271/49271 [==============================] - 12s 236us/step - loss: 0.2929 - acc: 0.8851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1fadbef28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "-jiJjPjYYmBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bc898954-a3c2-4f1f-9a81-a2b26d729cf1"
      },
      "cell_type": "code",
      "source": [
        "scores = model_MLP.evaluate(X_train, y_train)\n",
        "print(\"%s: %.2f%%\" % (model_MLP.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49271/49271 [==============================] - 9s 176us/step\n",
            "acc: 88.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YHXL4pO4Yl9o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred_class_MLP = model_MLP.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9XLIwDMjYl7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03edd4f6-ee3e-4144-96e7-6d0a026bb53e"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(y_test,y_pred_class_MLP)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8862640038967364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "7tjinLVFYl5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the confusion matrix\n",
        "confusion_matrix_test_mlp=metrics.confusion_matrix(y_test,y_pred_class_MLP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97wxX9NhYl2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c05157ab-0a7a-4e48-cf76-2ec50964517d"
      },
      "cell_type": "code",
      "source": [
        "Accuracy_Test=(confusion_matrix_test_mlp[0,0]+confusion_matrix_test_mlp[1,1])/(confusion_matrix_test_mlp[0,0]+confusion_matrix_test_mlp[0,1]+confusion_matrix_test_mlp[1,0]+confusion_matrix_test_mlp[1,1])\n",
        "TNR_Test= confusion_matrix_test_mlp[0,0]/(confusion_matrix_test_mlp[0,0] +confusion_matrix_test_mlp[0,1])\n",
        "TPR_Test= confusion_matrix_test_mlp[1,1]/(confusion_matrix_test_mlp[1,0] +confusion_matrix_test_mlp[1,1])\n",
        "\n",
        "print(\"Test TNR: \",TNR_Test)\n",
        "print(\"Test TPR: \",TPR_Test)\n",
        "print(\"Test Accuracy: \",Accuracy_Test)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test TNR:  0.9391597304795878\n",
            "Test TPR:  0.6464510332434861\n",
            "Test Accuracy:  0.8862640038967364\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}